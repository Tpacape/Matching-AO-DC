{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dateparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-511-8ea4b5ff86d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdateparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dateparser'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import yaml\n",
    "import dateparser\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# PDFMiner\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "# Collections\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "\n",
    "# Gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# creuser la présence de mot anglais\n",
    "\n",
    "# Import for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vars & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncharbit\\AppData\\Local\\Continuum\\anaconda3\\envs\\p36NLP\\lib\\site-packages\\ipykernel_launcher.py:11: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "AO_directory = \"data/input/AO\"\n",
    "DC_directory = \"data/input/DC\"\n",
    "\n",
    "AO_output_directory = \"data/output/AO\"\n",
    "AO_save_file = \"AO_save_file.csv\"\n",
    "\n",
    "DC_output_directory = \"data/output/DC\"\n",
    "DC_save_file = \"DC_save_file.csv\"\n",
    "\n",
    "confs_path = \"confs/config.yaml\"\n",
    "confs = yaml.load(open(confs_path, encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete french stopwords\n",
    "isDeleteFrenchStopwords = False\n",
    "\n",
    "# Delete english stopwords\n",
    "isDeleteEnglishStopwords = False \n",
    "\n",
    "#lower the word\n",
    "isLower = True\n",
    "\n",
    "#lemmatize the word\n",
    "isLemmatize = False\n",
    "\n",
    "perplexity=30\n",
    "\n",
    "possibilities = [False, True]\n",
    "\n",
    "perplexityPossibilities=[30, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract & Convert into PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf(path):\n",
    "    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    with open(path, \"rb\") as fp:\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=caching,\n",
    "                                      check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        text = retstr.getvalue()\n",
    "\n",
    "        fp.close()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def extract(directory):\n",
    "\n",
    "    # Reference variables\n",
    "    file_agg = list()\n",
    "\n",
    "    # Create list of candidate files\n",
    "    for root, subdirs, files in os.walk(directory):\n",
    "        folder_files = map(lambda x: os.path.join(root, x), files)\n",
    "        file_agg.extend(folder_files)\n",
    "\n",
    "    # Convert list to a pandas DataFrame\n",
    "    observations = pd.DataFrame(data=file_agg, columns=['file_path'])\n",
    "    observations['extension'] = observations['file_path'].apply(lambda x: os.path.splitext(x)[1])\n",
    "    observations['text'] = observations['file_path'].apply(convert_pdf)\n",
    "    \n",
    "    return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AO_observations = extract(AO_directory)\n",
    "DC_observations = extract(DC_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Extracted & Converted files into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(observations, output_directory, save_file):\n",
    "\n",
    "    output_path = os.path.join(output_directory, save_file)\n",
    "    observations.to_csv(path_or_buf=output_path, index_label='index')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(AO_observations, AO_output_directory, AO_save_file)\n",
    "#save(DC_observations, DC_output_directory, DC_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDimensionList (my_list) :\n",
    "    \n",
    "    my_list = [item for sublist in my_list for item in sublist]\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumn(start_index, end_index, sentences):\n",
    "\n",
    "        if start_index != 0:\n",
    "            start_index = start_index + 1\n",
    "        \n",
    "        if end_index == 0 :\n",
    "            text = sentences[start_index:]\n",
    "        else : \n",
    "            text = sentences[start_index:end_index]\n",
    "                        \n",
    "        return text \n",
    "    \n",
    "        text = [word_tokenize(w) for w in text]\n",
    "\n",
    "        my_text = []\n",
    "        for sent in text :\n",
    "            my_sent = []\n",
    "            for word in sent :\n",
    "                # Delete date or phone number\n",
    "                #if not len(re.findall(\"[a-zA-Z]\",word)) > 0: \n",
    "                #    continue\n",
    "                    \n",
    "                #lower the word\n",
    "                if isLower:    \n",
    "                    word = word.lower()\n",
    "                    \n",
    "                #lemmatize the word\n",
    "                if isLemmatize:\n",
    "                    word = nlp(word)[0].lemma_ \n",
    "                    \n",
    "                # Delete french stopwords\n",
    "                if isDeleteFrenchStopwords and (word in stopwords.words(\"french\")):\n",
    "                    continue\n",
    "                \n",
    "                # Delete english stopwords\n",
    "                if isDeleteEnglishStopwords and (word in stopwords.words(\"english\")):\n",
    "                    continue\n",
    "                   \n",
    "                #append (or not) the word to the \"sentence\"\n",
    "                if len(word) > 0:\n",
    "                    my_sent.append(word)\n",
    "\n",
    "            #append (or not) the sentence to the \"text\"\n",
    "            if len(my_sent) > 0:\n",
    "                my_text.append(my_sent)\n",
    "                \n",
    "        \n",
    "        return my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingAO(observations):  \n",
    "       \n",
    "    observations['sentences'] = \"\"\n",
    "    observations['sentences2'] = \"\"\n",
    "\n",
    "    for index, row in tqdm(observations.iterrows()):\n",
    "        \n",
    "        # Tokenization\n",
    "        sentences = []        \n",
    "        del_1 = \"\\n\"\n",
    "        delimiters = del_1 \n",
    "        paragraphs = [p for p in re.split(delimiters, row[\"text\"]) if p]\n",
    "        for paragraph in paragraphs:\n",
    "            sentences += sent_tokenize(paragraph)\n",
    "        observations.loc[index, \"sentences\"] = sentences\n",
    "        \n",
    "        observations.loc[index, \"sentences2\"] = getColumn(0, 0, sentences)\n",
    "\n",
    "        \n",
    "    return observations\n",
    "\n",
    "\n",
    "def preprocessingDC(observations):  \n",
    "    \n",
    "    observations['sentences'] = \"\"\n",
    "    observations['sentences2'] = \"\"\n",
    "    observations['skills'] = \"\"\n",
    "    observations['exp'] = \"\"\n",
    "    observations['formation'] = \"\"\n",
    "    \n",
    "        \n",
    "    for index, row in tqdm(observations.iterrows()):\n",
    "        \n",
    "        # Tokenization\n",
    "        sentences = []        \n",
    "        del_1 = \"JEMS - \\d.*.com\"\n",
    "        del_2 = \"Votre contact JEMS .* \\n\"\n",
    "        del_3 = \"\\n\"\n",
    "        #del_4 = \"\\/\"\n",
    "        delimiters = del_1 + '|' + del_2 + '|' + del_3# + '|' + del_4\n",
    "        paragraphs = [p for p in re.split(delimiters, row[\"text\"]) if p]\n",
    "        for paragraph in paragraphs:\n",
    "            sentences += sent_tokenize(paragraph)\n",
    "        observations.loc[index, 'sentences'] = sentences\n",
    "        \n",
    "        \n",
    "        # Get index\n",
    "        list_index_synthese = ['SYNTHESE DE COMPETENCES']\n",
    "        list_index_experience = ['EXPERIENCES PROFESSIONNELLES', 'EXPERIENCE PROFESSIONNELLE']\n",
    "        list_index_formation = ['FORMATIONS', 'FORMATION', \"DIPLOMES & FORMATIONS\"]\n",
    "        for i,e in enumerate(sentences):\n",
    "            if e in list_index_synthese:\n",
    "                index_synthese = i\n",
    "            elif e in list_index_experience:\n",
    "                index_experience = i\n",
    "            elif e in list_index_formation:\n",
    "                index_formation = i\n",
    "\n",
    "        \n",
    "        # Get Section By Section\n",
    "        dictionnaryIndex = {\n",
    "            \"skills\" : index_synthese,\n",
    "            \"exp\" : index_experience,\n",
    "            \"formation\" : index_formation}       \n",
    "        listofTuples = sorted(dictionnaryIndex.items(), key=lambda x: x[1])\n",
    "        for i in range(len(listofTuples)):\n",
    "            k = listofTuples[i][0]\n",
    "            v = int(listofTuples[i][1])\n",
    "            \n",
    "            v2 = 0\n",
    "            if i < len(listofTuples) - 1:\n",
    "                k2 = listofTuples[i+1][0]\n",
    "                v2 = int(listofTuples[i+1][1])\n",
    "            \n",
    "            \n",
    "            observations.loc[index, k] = getColumn(v, v2, sentences)\n",
    "            \n",
    "        observations.loc[index, \"sentences2\"] = getColumn(0, 0, sentences)\n",
    "        \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 87.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#AO_observations = preprocessingAO(AO_observations)\n",
    "DC_observations = preprocessingDC(DC_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>extension</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences2</th>\n",
       "      <th>skills</th>\n",
       "      <th>exp</th>\n",
       "      <th>formation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences  JEMS Gr...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\nSenior PMO  \\n\\nS...</td>\n",
       "      <td>[1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...</td>\n",
       "      <td>[1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...</td>\n",
       "      <td>[-  Compétences fonctionnelles :, -  Conduite ...</td>\n",
       "      <td>[06/2018 – 12/18 – Consultante – JEMS Group – ...</td>\n",
       "      <td>[❖  2018 – Design thinking project management ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/input/DC\\Dossier de compétences de JEMS ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>RHA \\n\\nConsultant BIG DATA \\n\\nSYNTHESE DE CO...</td>\n",
       "      <td>[RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...</td>\n",
       "      <td>[RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...</td>\n",
       "      <td>[  Savoir-faire :, -  Connaissance de l’écosys...</td>\n",
       "      <td>[Depuis Avril 2018       JEMS Datafactory, ---...</td>\n",
       "      <td>[Diplôme d’ingénieur en génie mathématique opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS  - ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nBSE \\n\\nConsultant Dat...</td>\n",
       "      <td>[1, BSE, Consultant Data Sénior, Développement...</td>\n",
       "      <td>[1, BSE, Consultant Data Sénior, Développement...</td>\n",
       "      <td>[Scikit-learn, pandas , pyTorch/ fastai, Keras...</td>\n",
       "      <td>[                                             ...</td>\n",
       "      <td>[2006, 1987, Master2 Mathématiques Fondamental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/input/DC\\Dossier de compétences JEMS -  ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n...</td>\n",
       "      <td>[Ingénieur d’études confirmé ASP.Net / C#, 7 a...</td>\n",
       "      <td>[Ingénieur d’études confirmé ASP.Net / C#, 7 a...</td>\n",
       "      <td>[SonarQube, Serveur d’application :   IIS, Apa...</td>\n",
       "      <td>[Mai 2014 à Décembre 2018 – Préfecture de RABA...</td>\n",
       "      <td>[2014/2016  :  MASTER Scientifique (Msc) Optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/input/DC\\Dossier de compétences JEMS -  ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...</td>\n",
       "      <td>[1, ., Consultant Technico-Fonctionnel, SYNTHE...</td>\n",
       "      <td>[1, ., Consultant Technico-Fonctionnel, SYNTHE...</td>\n",
       "      <td>[❖  Finance, Finance de marché, Gestion d’acti...</td>\n",
       "      <td>[ AXA Investment Partners – Front office Analy...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - A...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         \\n\\n \\n\\...</td>\n",
       "      <td>[1, Développeur Fullstack, 6 années d’expérien...</td>\n",
       "      <td>[1, Développeur Fullstack, 6 années d’expérien...</td>\n",
       "      <td>[Métiers, Fonctionnelles, Etudes transverses f...</td>\n",
       "      <td>[Novembre 2017 – Mars 2019, Développeur Full S...</td>\n",
       "      <td>[\f",
       "2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - A...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nASA \\n\\nDATA SCIENTIST...</td>\n",
       "      <td>[1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[Domaines de compétences, Intelligence Artific...</td>\n",
       "      <td>[&lt;IN-TEAM/ startup&gt;, Domaine de compétences : ...</td>\n",
       "      <td>[  2011-2013 :   Diplôme national d’ingénieur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - D...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\nDK \\n\\nConsultant Big Data \\n\\...</td>\n",
       "      <td>[1, DK, Consultant Big Data, SYNTHESE DE COMPE...</td>\n",
       "      <td>[1, DK, Consultant Big Data, SYNTHESE DE COMPE...</td>\n",
       "      <td>[❖  Compétences techniques :, - Programmation ...</td>\n",
       "      <td>[Février 2018 – Février 2019 : Data Scientist ...</td>\n",
       "      <td>[❖  Juin 2001:, Baccalauréat grec ''Apolytirio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - E...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         ...</td>\n",
       "      <td>[1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...</td>\n",
       "      <td>[1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...</td>\n",
       "      <td>[  Compétences techniques, •  Langages : Pytho...</td>\n",
       "      <td>[2019 – Consultant Big Data (Consultant Jems) ...</td>\n",
       "      <td>[OBJECTIFS :, ➢ Définition du périmètre du Big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - E...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n...</td>\n",
       "      <td>[1, EZA, Consultant DevOps, SYNTHESE DE COMPET...</td>\n",
       "      <td>[1, EZA, Consultant DevOps, SYNTHESE DE COMPET...</td>\n",
       "      <td>[❖  Compétences techniques :, Système OS :    ...</td>\n",
       "      <td>[Octobre 2018 –  Janvier 2019 : Consultant DEV...</td>\n",
       "      <td>[❖  Participation au développement de nouvelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - H...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nH \\n\\nData Scientist  ...</td>\n",
       "      <td>[1, H, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[1, H, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[❖  Compétences techniques : Python(Tensorflow...</td>\n",
       "      <td>[Mai – Novembre 2018 – Data Scientist – BNP Pa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - H...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\nExpert Big Data  ...</td>\n",
       "      <td>[1, Expert Big Data, SYNTHESE DE COMPETENCES, ...</td>\n",
       "      <td>[1, Expert Big Data, SYNTHESE DE COMPETENCES, ...</td>\n",
       "      <td>[Compétences fonctionnelles :,   Big Data : Ma...</td>\n",
       "      <td>[Depuis Octobre 2015 : Jems group, Expertise B...</td>\n",
       "      <td>[Certification HDP 2.4, Formations BigData (Ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - I...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nIS  \\n\\nData Scientist...</td>\n",
       "      <td>[1, IS, Data Scientist, SYNTHESE DE COMPETENCE...</td>\n",
       "      <td>[1, IS, Data Scientist, SYNTHESE DE COMPETENCE...</td>\n",
       "      <td>[❖  Compétences techniques :,           • Pyth...</td>\n",
       "      <td>[1er sept. 2018  –  1er mars 2019 Data Scienti...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - N...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nNZE \\n\\nCONSULTANT TAL...</td>\n",
       "      <td>[1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...</td>\n",
       "      <td>[1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...</td>\n",
       "      <td>[MISE EN ŒUVRE ET MAINTENANCE D’APPLICATIONS Q...</td>\n",
       "      <td>[DE NOVEMBRE 2017 A AJOURD’HUI – VEOLIA WATER ...</td>\n",
       "      <td>[Formation ServiceNow, JIRA, Formation Google ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - P...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nP \\n\\nData Scientist \\...</td>\n",
       "      <td>[1, P, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[1, P, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[❖  Compétences techniques :, ❖, - Programmati...</td>\n",
       "      <td>[28/06/2018 – 31/12/2018 – DATA SCIENTIST – BN...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n \\n\\nArchitecte B...</td>\n",
       "      <td>[1, Architecte Big Data / IA / Cloud, SYNTHESE...</td>\n",
       "      <td>[1, Architecte Big Data / IA / Cloud, SYNTHESE...</td>\n",
       "      <td>[Systèmes et développement :, Big Data:, BI:, ...</td>\n",
       "      <td>[02/2018 à 31/12/2018   Architecte Entreprise ...</td>\n",
       "      <td>[MIT Sloan &amp; MIT CSAIL Artificial Intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nRDB \\n\\n \\n\\nConsultan...</td>\n",
       "      <td>[1, RDB, Consultant AMOA, Chef de projet fonct...</td>\n",
       "      <td>[1, RDB, Consultant AMOA, Chef de projet fonct...</td>\n",
       "      <td>[Management d’équipe et pilotage, Coordination...</td>\n",
       "      <td>[Product Owner, Juin 2014 – Mai 2017, Bionexo ...</td>\n",
       "      <td>[Gestion, stratégie et économie d’entreprise (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nRTS \\n\\nConsultant Sen...</td>\n",
       "      <td>[1, RTS, Consultant Senior Java/Jee – Batch - ...</td>\n",
       "      <td>[1, RTS, Consultant Senior Java/Jee – Batch - ...</td>\n",
       "      <td>[Base de données, PL SQL, Oracle, Sybase, SQL ...</td>\n",
       "      <td>[SOCIETE GENERALE (VAL DE FONTENAY)  Mars 2018...</td>\n",
       "      <td>[2018 – 2019, Formation Certifiante Expert Dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS - Z...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n         \\n\\n \\n\\nZAO \\n\\...</td>\n",
       "      <td>[1, ZAO, Data Scientist / Engineer, SYNTHESE D...</td>\n",
       "      <td>[1, ZAO, Data Scientist / Engineer, SYNTHESE D...</td>\n",
       "      <td>[Langages de programmation :, Python (Niveau a...</td>\n",
       "      <td>[Depuis le 11/2018 à Aujourd’hui :, Co-fondate...</td>\n",
       "      <td>[Université Panthéon ASSAS/Université Paris DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS -A.pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...</td>\n",
       "      <td>[1, ., Consultant Big Data, SYNTHESE DE COMPET...</td>\n",
       "      <td>[1, ., Consultant Big Data, SYNTHESE DE COMPET...</td>\n",
       "      <td>[Compétences techniques :, * OS : Windows, Lin...</td>\n",
       "      <td>[Contexte client : au sein du département Data...</td>\n",
       "      <td>[Formation Big Data | Hadoop | Spark scala, « ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS G M...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n \\n\\n \\n\\n \\n\\nChef de Projet Data Manage...</td>\n",
       "      <td>[Chef de Projet Data Management, +10y experien...</td>\n",
       "      <td>[Chef de Projet Data Management, +10y experien...</td>\n",
       "      <td>[Conduite de projet, -, -, -, -, -, -, Définit...</td>\n",
       "      <td>[Loxam – 2013 - 2018, Missions :, -  Responsab...</td>\n",
       "      <td>[-  MASTER II MIAGE : Ingénierie des Systèmes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/input/DC\\Dossier de Compétences JEMS Gro...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n          \\n\\n \\n\\n \\n\\nConsultant Big Da...</td>\n",
       "      <td>[Consultant Big Data, 1, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[Consultant Big Data, 1, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[❖  Savoir-faire :, -  Recueil du besoin métie...</td>\n",
       "      <td>[Depuis Avril 2017 : SEPHORA, Chef de Projet B...</td>\n",
       "      <td>[-  Test des différentes solutions d’intégrati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path extension  \\\n",
       "0   data/input/DC\\Dossier de Compétences  JEMS Gr...      .pdf   \n",
       "1   data/input/DC\\Dossier de compétences de JEMS ...      .pdf   \n",
       "2   data/input/DC\\Dossier de Compétences JEMS  - ...      .pdf   \n",
       "3   data/input/DC\\Dossier de compétences JEMS -  ...      .pdf   \n",
       "4   data/input/DC\\Dossier de compétences JEMS -  ...      .pdf   \n",
       "5   data/input/DC\\Dossier de Compétences JEMS - A...      .pdf   \n",
       "6   data/input/DC\\Dossier de Compétences JEMS - A...      .pdf   \n",
       "7   data/input/DC\\Dossier de Compétences JEMS - D...      .pdf   \n",
       "8   data/input/DC\\Dossier de Compétences JEMS - E...      .pdf   \n",
       "9   data/input/DC\\Dossier de Compétences JEMS - E...      .pdf   \n",
       "10  data/input/DC\\Dossier de Compétences JEMS - H...      .pdf   \n",
       "11  data/input/DC\\Dossier de Compétences JEMS - H...      .pdf   \n",
       "12  data/input/DC\\Dossier de Compétences JEMS - I...      .pdf   \n",
       "13  data/input/DC\\Dossier de Compétences JEMS - N...      .pdf   \n",
       "14  data/input/DC\\Dossier de Compétences JEMS - P...      .pdf   \n",
       "15  data/input/DC\\Dossier de Compétences JEMS - R...      .pdf   \n",
       "16  data/input/DC\\Dossier de Compétences JEMS - R...      .pdf   \n",
       "17  data/input/DC\\Dossier de Compétences JEMS - R...      .pdf   \n",
       "18  data/input/DC\\Dossier de Compétences JEMS - Z...      .pdf   \n",
       "19  data/input/DC\\Dossier de Compétences JEMS -A.pdf      .pdf   \n",
       "20  data/input/DC\\Dossier de Compétences JEMS G M...      .pdf   \n",
       "21  data/input/DC\\Dossier de Compétences JEMS Gro...      .pdf   \n",
       "\n",
       "                                                 text  \\\n",
       "0   1 \\n\\n         \\n\\n \\n\\n \\n\\nSenior PMO  \\n\\nS...   \n",
       "1   RHA \\n\\nConsultant BIG DATA \\n\\nSYNTHESE DE CO...   \n",
       "2   1 \\n\\n         \\n\\n \\n\\nBSE \\n\\nConsultant Dat...   \n",
       "3    \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n...   \n",
       "4   1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...   \n",
       "5   1 \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         \\n\\n \\n\\...   \n",
       "6   1 \\n\\n         \\n\\n \\n\\nASA \\n\\nDATA SCIENTIST...   \n",
       "7   1 \\n\\n \\n\\n \\n\\nDK \\n\\nConsultant Big Data \\n\\...   \n",
       "8   1 \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         ...   \n",
       "9   1 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n...   \n",
       "10  1 \\n\\n         \\n\\n \\n\\nH \\n\\nData Scientist  ...   \n",
       "11  1 \\n\\n         \\n\\n \\n\\n \\n\\nExpert Big Data  ...   \n",
       "12  1 \\n\\n         \\n\\n \\n\\nIS  \\n\\nData Scientist...   \n",
       "13  1 \\n\\n         \\n\\n \\n\\nNZE \\n\\nCONSULTANT TAL...   \n",
       "14  1 \\n\\n         \\n\\n \\n\\nP \\n\\nData Scientist \\...   \n",
       "15  1 \\n\\n         \\n\\n \\n\\n \\n\\n \\n\\nArchitecte B...   \n",
       "16  1 \\n\\n         \\n\\n \\n\\nRDB \\n\\n \\n\\nConsultan...   \n",
       "17  1 \\n\\n         \\n\\n \\n\\nRTS \\n\\nConsultant Sen...   \n",
       "18  1 \\n\\n \\n\\n \\n\\n \\n\\n         \\n\\n \\n\\nZAO \\n\\...   \n",
       "19  1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...   \n",
       "20   \\n\\n \\n\\n \\n\\n \\n\\nChef de Projet Data Manage...   \n",
       "21   \\n\\n          \\n\\n \\n\\n \\n\\nConsultant Big Da...   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   [1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...   \n",
       "1   [RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...   \n",
       "2   [1, BSE, Consultant Data Sénior, Développement...   \n",
       "3   [Ingénieur d’études confirmé ASP.Net / C#, 7 a...   \n",
       "4   [1, ., Consultant Technico-Fonctionnel, SYNTHE...   \n",
       "5   [1, Développeur Fullstack, 6 années d’expérien...   \n",
       "6   [1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...   \n",
       "7   [1, DK, Consultant Big Data, SYNTHESE DE COMPE...   \n",
       "8   [1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...   \n",
       "9   [1, EZA, Consultant DevOps, SYNTHESE DE COMPET...   \n",
       "10  [1, H, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "11  [1, Expert Big Data, SYNTHESE DE COMPETENCES, ...   \n",
       "12  [1, IS, Data Scientist, SYNTHESE DE COMPETENCE...   \n",
       "13  [1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...   \n",
       "14  [1, P, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "15  [1, Architecte Big Data / IA / Cloud, SYNTHESE...   \n",
       "16  [1, RDB, Consultant AMOA, Chef de projet fonct...   \n",
       "17  [1, RTS, Consultant Senior Java/Jee – Batch - ...   \n",
       "18  [1, ZAO, Data Scientist / Engineer, SYNTHESE D...   \n",
       "19  [1, ., Consultant Big Data, SYNTHESE DE COMPET...   \n",
       "20  [Chef de Projet Data Management, +10y experien...   \n",
       "21  [Consultant Big Data, 1, SYNTHESE DE COMPETENC...   \n",
       "\n",
       "                                           sentences2  \\\n",
       "0   [1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...   \n",
       "1   [RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...   \n",
       "2   [1, BSE, Consultant Data Sénior, Développement...   \n",
       "3   [Ingénieur d’études confirmé ASP.Net / C#, 7 a...   \n",
       "4   [1, ., Consultant Technico-Fonctionnel, SYNTHE...   \n",
       "5   [1, Développeur Fullstack, 6 années d’expérien...   \n",
       "6   [1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...   \n",
       "7   [1, DK, Consultant Big Data, SYNTHESE DE COMPE...   \n",
       "8   [1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...   \n",
       "9   [1, EZA, Consultant DevOps, SYNTHESE DE COMPET...   \n",
       "10  [1, H, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "11  [1, Expert Big Data, SYNTHESE DE COMPETENCES, ...   \n",
       "12  [1, IS, Data Scientist, SYNTHESE DE COMPETENCE...   \n",
       "13  [1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...   \n",
       "14  [1, P, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "15  [1, Architecte Big Data / IA / Cloud, SYNTHESE...   \n",
       "16  [1, RDB, Consultant AMOA, Chef de projet fonct...   \n",
       "17  [1, RTS, Consultant Senior Java/Jee – Batch - ...   \n",
       "18  [1, ZAO, Data Scientist / Engineer, SYNTHESE D...   \n",
       "19  [1, ., Consultant Big Data, SYNTHESE DE COMPET...   \n",
       "20  [Chef de Projet Data Management, +10y experien...   \n",
       "21  [Consultant Big Data, 1, SYNTHESE DE COMPETENC...   \n",
       "\n",
       "                                               skills  \\\n",
       "0   [-  Compétences fonctionnelles :, -  Conduite ...   \n",
       "1   [  Savoir-faire :, -  Connaissance de l’écosys...   \n",
       "2   [Scikit-learn, pandas , pyTorch/ fastai, Keras...   \n",
       "3   [SonarQube, Serveur d’application :   IIS, Apa...   \n",
       "4   [❖  Finance, Finance de marché, Gestion d’acti...   \n",
       "5   [Métiers, Fonctionnelles, Etudes transverses f...   \n",
       "6   [Domaines de compétences, Intelligence Artific...   \n",
       "7   [❖  Compétences techniques :, - Programmation ...   \n",
       "8   [  Compétences techniques, •  Langages : Pytho...   \n",
       "9   [❖  Compétences techniques :, Système OS :    ...   \n",
       "10  [❖  Compétences techniques : Python(Tensorflow...   \n",
       "11  [Compétences fonctionnelles :,   Big Data : Ma...   \n",
       "12  [❖  Compétences techniques :,           • Pyth...   \n",
       "13  [MISE EN ŒUVRE ET MAINTENANCE D’APPLICATIONS Q...   \n",
       "14  [❖  Compétences techniques :, ❖, - Programmati...   \n",
       "15  [Systèmes et développement :, Big Data:, BI:, ...   \n",
       "16  [Management d’équipe et pilotage, Coordination...   \n",
       "17  [Base de données, PL SQL, Oracle, Sybase, SQL ...   \n",
       "18  [Langages de programmation :, Python (Niveau a...   \n",
       "19  [Compétences techniques :, * OS : Windows, Lin...   \n",
       "20  [Conduite de projet, -, -, -, -, -, -, Définit...   \n",
       "21  [❖  Savoir-faire :, -  Recueil du besoin métie...   \n",
       "\n",
       "                                                  exp  \\\n",
       "0   [06/2018 – 12/18 – Consultante – JEMS Group – ...   \n",
       "1   [Depuis Avril 2018       JEMS Datafactory, ---...   \n",
       "2   [                                             ...   \n",
       "3   [Mai 2014 à Décembre 2018 – Préfecture de RABA...   \n",
       "4   [ AXA Investment Partners – Front office Analy...   \n",
       "5   [Novembre 2017 – Mars 2019, Développeur Full S...   \n",
       "6   [<IN-TEAM/ startup>, Domaine de compétences : ...   \n",
       "7   [Février 2018 – Février 2019 : Data Scientist ...   \n",
       "8   [2019 – Consultant Big Data (Consultant Jems) ...   \n",
       "9   [Octobre 2018 –  Janvier 2019 : Consultant DEV...   \n",
       "10  [Mai – Novembre 2018 – Data Scientist – BNP Pa...   \n",
       "11  [Depuis Octobre 2015 : Jems group, Expertise B...   \n",
       "12  [1er sept. 2018  –  1er mars 2019 Data Scienti...   \n",
       "13  [DE NOVEMBRE 2017 A AJOURD’HUI – VEOLIA WATER ...   \n",
       "14  [28/06/2018 – 31/12/2018 – DATA SCIENTIST – BN...   \n",
       "15  [02/2018 à 31/12/2018   Architecte Entreprise ...   \n",
       "16  [Product Owner, Juin 2014 – Mai 2017, Bionexo ...   \n",
       "17  [SOCIETE GENERALE (VAL DE FONTENAY)  Mars 2018...   \n",
       "18  [Depuis le 11/2018 à Aujourd’hui :, Co-fondate...   \n",
       "19  [Contexte client : au sein du département Data...   \n",
       "20  [Loxam – 2013 - 2018, Missions :, -  Responsab...   \n",
       "21  [Depuis Avril 2017 : SEPHORA, Chef de Projet B...   \n",
       "\n",
       "                                            formation  \n",
       "0   [❖  2018 – Design thinking project management ...  \n",
       "1   [Diplôme d’ingénieur en génie mathématique opt...  \n",
       "2   [2006, 1987, Master2 Mathématiques Fondamental...  \n",
       "3   [2014/2016  :  MASTER Scientifique (Msc) Optio...  \n",
       "4                                                  []  \n",
       "5                                                [\n",
       "2]  \n",
       "6   [  2011-2013 :   Diplôme national d’ingénieur...  \n",
       "7   [❖  Juin 2001:, Baccalauréat grec ''Apolytirio...  \n",
       "8   [OBJECTIFS :, ➢ Définition du périmètre du Big...  \n",
       "9   [❖  Participation au développement de nouvelle...  \n",
       "10                                                 []  \n",
       "11  [Certification HDP 2.4, Formations BigData (Ce...  \n",
       "12                                                 []  \n",
       "13  [Formation ServiceNow, JIRA, Formation Google ...  \n",
       "14                                                 []  \n",
       "15  [MIT Sloan & MIT CSAIL Artificial Intelligence...  \n",
       "16  [Gestion, stratégie et économie d’entreprise (...  \n",
       "17  [2018 – 2019, Formation Certifiante Expert Dev...  \n",
       "18  [Université Panthéon ASSAS/Université Paris DA...  \n",
       "19  [Formation Big Data | Hadoop | Spark scala, « ...  \n",
       "20  [-  MASTER II MIAGE : Ingénierie des Systèmes ...  \n",
       "21  [-  Test des différentes solutions d’intégrati...  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreciseDate(text) : \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.split(\"[^A-Za-z0-9é]+\", text)\n",
    "    \n",
    "    year_pattern1 = '((19|20)\\d{2})'\n",
    "    year_pattern2 = '(\\d{2})'\n",
    "\n",
    "    month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "    month_pattern2 = \"(jan|fev|fév|mar|avr|mai|juin|juil|aout|sep|oct|nov|dec|déc)\"\n",
    "    month_pattern3 = \"(janvier|février|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre|décembre) \"\n",
    "    month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # YYYY -> YYYY\n",
    "    for i in range(len(text)):\n",
    "        if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(year_pattern1, text[i - 1])) :\n",
    "            results.append(\"01/\" + \"01/\" + text[i - 1])\n",
    "            results.append(\"01/\" + \"01/\" + text[i])\n",
    "            \n",
    "    # 01/2018 -> 02/2018 or Jan/2018 -> Fév/2018\n",
    "    if len(results) == 0:\n",
    "        for i in range(len(text)):   \n",
    "            if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(month_pattern, text[i - 1])) :\n",
    "                results.append(\"01/\" + text[i - 1] + \"/\" + text[i])\n",
    "\n",
    "    # 2018 \n",
    "    if len(results) == 0:\n",
    "        for i in range(len(text)):   \n",
    "            if (i == 0) and (re.match(year_pattern1, text[i])) :\n",
    "                results.append(\"01/\" + \"01/\" + text[i])\n",
    "\n",
    "\n",
    "\n",
    "    if len(results) == 1 :\n",
    "        results.append(\"Aujourd'hui\")\n",
    "        \n",
    "\n",
    "    for i in range(len(results)) :\n",
    "        results[i] = dateparser.parse(results[i])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def getKeyDate(sentences):\n",
    "    \n",
    "    keydate = []\n",
    "    for sentence in sentences : \n",
    "        if len(getPreciseDate(sentence)) > 0 : \n",
    "            keydate.append(sentence)\n",
    "\n",
    "    \n",
    "    if len(keydate) <= 0 :\n",
    "        raise Exception(\"No professionnal experience find\")\n",
    "            \n",
    "    return keydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllInformations(keydate, df_):\n",
    "    \n",
    "    \n",
    "    for i in range(len(keydate) - 1) : \n",
    "        dateDebut = DC_observations.loc[DC, \"exp\"].index(keydate[i])\n",
    "        dateFin = DC_observations.loc[DC, \"exp\"].index(keydate[i + 1])\n",
    "\n",
    "        df_['Mission'] = df_['Mission'].astype(object)\n",
    "        df_['MissionString'] = df_['MissionString'].astype(object)\n",
    "\n",
    "        df_.at[i, \"Mission\"] = DC_observations.loc[DC, \"exp\"][dateDebut:dateFin]\n",
    "        df_.at[i, \"MissionString\"] = \" \".join(df_.at[i, \"Mission\"])\n",
    "\n",
    "        df_.loc[i, \"DD\"] = keydate[i]\n",
    "        df_.loc[i, \"DF\"] = keydate[i + 1]\n",
    "\n",
    "        df_.loc[i, \"iDD\"] = dateDebut\n",
    "        df_.loc[i, \"iDF\"] = dateFin\n",
    "        \n",
    "\n",
    "    for index, row in df_.iterrows():\n",
    "    \n",
    "        text = str(df_.loc[index, \"MissionString\"])\n",
    "\n",
    "        pattern = getPattern()\n",
    "        \n",
    "        # Get start date & end date\n",
    "        mydates = getPreciseDate(text)       \n",
    "\n",
    "        # Parse into real date\n",
    "        df_.loc[index, \"DD\"] = mydates[0]\n",
    "        df_.loc[index, \"DF\"] = mydates[1]\n",
    "\n",
    "    df_['DD'] = pd.to_datetime(df_['DD'])\n",
    "    df_['DF'] = pd.to_datetime(df_['DF'])\n",
    "    df_['result'] = df_[\"DF\"] - df_['DD']\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(string_to_search, term):\n",
    "    try:\n",
    "        term = r'\\b' + re.escape(term) + r'\\b'\n",
    "        regular_expression = re.compile(term, re.IGNORECASE)\n",
    "        result = re.findall(regular_expression, string_to_search)\n",
    "        return len(result)\n",
    "    except Exception:\n",
    "        logging.error('Error occurred during regex search')\n",
    "        return 0\n",
    "    \n",
    "def extract_skills(resume_text, extractor, items_of_interest):\n",
    "    potential_skills_dict = dict()\n",
    "    matched_skills = []\n",
    "\n",
    "    # Translate the YAML file into a dic : potential_skills_dict\n",
    "    for skill_input in items_of_interest:\n",
    "\n",
    "        # Format list inputs\n",
    "        if type(skill_input) is list and len(skill_input) >= 1:\n",
    "            potential_skills_dict[skill_input[0]] = skill_input\n",
    "\n",
    "        # Format string inputs\n",
    "        elif type(skill_input) is str:\n",
    "            potential_skills_dict[skill_input] = [skill_input]\n",
    "        else:\n",
    "            logging.warn('Unknown skill listing type: {}. Please format as either a single string or a list of strings'\n",
    "                         ''.format(skill_input))\n",
    "\n",
    "    for (skill_name, skill_alias_list) in potential_skills_dict.items():\n",
    "\n",
    "        skill_matches = 0\n",
    "        # Iterate through aliases\n",
    "        for skill_alias in skill_alias_list:\n",
    "            # Add the number of matches for each alias\n",
    "            skill_matches += term_count(resume_text.lower(), skill_alias.lower())\n",
    "            \n",
    "\n",
    "        # If at least one alias is found, add skill name to set of skills\n",
    "        if skill_matches > 0:\n",
    "            matched_skills.append(skill_name)\n",
    "\n",
    "    return matched_skills\n",
    "\n",
    "def extract_fields(df):\n",
    "    for extractor, items_of_interest in confs['extractors'].items():\n",
    "        df_[extractor] = df_['MissionString'].apply(lambda x: extract_skills(x, extractor, items_of_interest))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtmp = [] \n",
    "\n",
    "for DC in range(len(DC_observations)) :\n",
    "#for DC in range(0,1) :\n",
    "\n",
    "    # Get all key date from the experiences\n",
    "    keydate = getKeyDate(DC_observations.loc[DC, \"exp\"])\n",
    "\n",
    "    # Create a tmp dataframe        \n",
    "    df_ = pd.DataFrame(index=np.arange(len(keydate) -1), columns=[\"Mission\", \"MissionString\",  \"DD\", \"DF\", \"iDD\", \"iDF\"])\n",
    "    df_ = df_.fillna(0) # with 0s rather than NaNs\n",
    "    \n",
    "    df_ = getAllInformations(keydate, df_)\n",
    "\n",
    "    df_ = extract_fields(df_)\n",
    "\n",
    "    listtmp.append(df_.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mission</th>\n",
       "      <th>MissionString</th>\n",
       "      <th>DD</th>\n",
       "      <th>DF</th>\n",
       "      <th>iDD</th>\n",
       "      <th>iDF</th>\n",
       "      <th>result</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Platforms</th>\n",
       "      <th>Database</th>\n",
       "      <th>...</th>\n",
       "      <th>DataVisualisation</th>\n",
       "      <th>DevOps</th>\n",
       "      <th>Cloud</th>\n",
       "      <th>BackProgramming</th>\n",
       "      <th>FrontProgramming</th>\n",
       "      <th>Methodologie</th>\n",
       "      <th>MachineLearning</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Langages</th>\n",
       "      <th>Open-source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Mai 2014 à Décembre 2018 – Préfecture de RABA...</td>\n",
       "      <td>Mai 2014 à Décembre 2018 – Préfecture de RABAT...</td>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1675 days</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Windows]</td>\n",
       "      <td>[SQL]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[C, .NET, HTML]</td>\n",
       "      <td>[HTML]</td>\n",
       "      <td>[Agile]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Janvier 2013 à janvier 2016 – Formateur Vacat...</td>\n",
       "      <td>Janvier 2013 à janvier 2016 – Formateur Vacata...</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>1095 days</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[SQL]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[java, C]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Mission  \\\n",
       "0  [Mai 2014 à Décembre 2018 – Préfecture de RABA...   \n",
       "1  [Janvier 2013 à janvier 2016 – Formateur Vacat...   \n",
       "\n",
       "                                       MissionString         DD         DF  \\\n",
       "0  Mai 2014 à Décembre 2018 – Préfecture de RABAT... 2014-05-01 2018-12-01   \n",
       "1  Janvier 2013 à janvier 2016 – Formateur Vacata... 2013-01-01 2016-01-01   \n",
       "\n",
       "   iDD  iDF    result Experience  Platforms Database  ... DataVisualisation  \\\n",
       "0    0   34 1675 days         []  [Windows]    [SQL]  ...                []   \n",
       "1   34   39 1095 days         []         []    [SQL]  ...                []   \n",
       "\n",
       "  DevOps Cloud  BackProgramming FrontProgramming Methodologie MachineLearning  \\\n",
       "0     []    []  [C, .NET, HTML]           [HTML]      [Agile]              []   \n",
       "1     []    []        [java, C]               []           []              []   \n",
       "\n",
       "  Mobile Langages Open-source  \n",
       "0     []       []          []  \n",
       "1     []       []          []  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "listtmp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Windows': Timedelta('1675 days 00:00:00'),\n",
       " 'SQL': Timedelta('2770 days 00:00:00'),\n",
       " 'C': Timedelta('2770 days 00:00:00'),\n",
       " '.NET': Timedelta('1675 days 00:00:00'),\n",
       " 'HTML': Timedelta('3350 days 00:00:00'),\n",
       " 'Agile': Timedelta('1675 days 00:00:00'),\n",
       " 'java': Timedelta('1095 days 00:00:00')}"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = listtmp[i]\n",
    "\n",
    "skills = {}\n",
    "\n",
    "for index, row in df_.iterrows():\n",
    "    for col in df_.loc[:, \"Experience\":] :\n",
    "        \n",
    "        items = df_.at[index, col]\n",
    "        if len(items) == 0 : \n",
    "            continue\n",
    "            \n",
    "        for item in items:\n",
    "            if not item in skills : \n",
    "                skills[item] = df_.at[index, \"result\"]\n",
    "            else :\n",
    "                skills[item] += df_.at[index, \"result\"]\n",
    "                \n",
    "                \n",
    "\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_ = pd.DataFrame(index=np.arange(1))\n",
    "df2_ = df2_.fillna(0) # with 0s rather than NaNs\n",
    "    \n",
    "for key, value in skills.items() :\n",
    "    if value.days < 0 : \n",
    "        df2_[key] = \"Junior\"\n",
    "    else :\n",
    "        df2_[key] = \"Senior\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Windows</th>\n",
       "      <th>Tableau</th>\n",
       "      <th>Python</th>\n",
       "      <th>SQL</th>\n",
       "      <th>Oracle</th>\n",
       "      <th>java</th>\n",
       "      <th>MySQL</th>\n",
       "      <th>HTML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Windows Tableau  Python     SQL  Oracle    java   MySQL    HTML\n",
       "0  Junior  Senior  Senior  Junior  Junior  Junior  Senior  Senior"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Display(observations, column, threshold) :\n",
    "\n",
    "    allTexts = reduceDimensionList(observations[column].tolist())\n",
    "    allTexts = reduceDimensionList(allTexts)\n",
    "    \n",
    "    \n",
    "    # Display len of words\n",
    "    test = [len(w) for w in allTexts]\n",
    "    print(\"Display len of words : \")\n",
    "    print(test)\n",
    "    plt.hist(test)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Write word with len > 15\n",
    "    test = [w for w in allTexts if len(w)>15]\n",
    "    if (len(test) > 0):\n",
    "        print(\"Display words where len>15 : \")\n",
    "        print(test)\n",
    "\n",
    "\n",
    "    # Write & Display word frequency >= threshold\n",
    "    test = Counter(allTexts)\n",
    "    test = {x : test[x] for x in test if test[x] >= threshold}\n",
    "    \n",
    "    print(\"Display frequency of words : \")\n",
    "    print(test)\n",
    "    \n",
    "    labels, values = zip(*test.items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.xticks(indexes + width * 0.5, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display(AO_observations, 'sentences2', 5)\n",
    "#Display(DC_observations, 'skills', 4)\n",
    "#Display(DC_observations, 'exp',20)\n",
    "#Display(DC_observations, 'formation', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#listToReduce = AO_observations['sentences2'].tolist() + DC_observations['sentences2'].tolist()\n",
    "\n",
    "#corpus = reduceDimensionList(listToReduce)\n",
    "#model = word2vec.Word2Vec(corpus, min_count=15, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    # Get words\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    # Get X & Y values\n",
    "    tsne_model = TSNE(perplexity=perplexity, init='pca', n_iter=5000, random_state=0)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "\n",
    "    matrix = np.column_stack((x, y))\n",
    "    clusters_number = 5\n",
    "    kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    assigned_clusters = kclusterer.cluster(matrix, assign_clusters=True)\n",
    "\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, clusters_number))\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i], c=colors[assigned_clusters[i]])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "        \n",
    "        \n",
    "    title = \"Mincount_10\" \\\n",
    "    + \" Files_\" + str(len(DC_observations) + len(AO_observations)) \\\n",
    "    + \" Perplexity\" + str(perplexity) \\\n",
    "    + \" FrenchStopword_\" + str(isDeleteFrenchStopwords) \\\n",
    "    + \" EnglishStopword_\" + str(isDeleteEnglishStopwords) \\\n",
    "    + \" Lower_\" + str(isLower) \\\n",
    "    + \" Lemmatization_\" + str(isLemmatize) \\\n",
    "    + \".png\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "    plt.savefig(title, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tsne_plot(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AO_extract = extract(AO_directory)\n",
    "DC_extract = extract(DC_directory)\n",
    "\n",
    "for perplexity in perplexityPossibilities :\n",
    "    for isDeleteFrenchStopwords in possibilities :\n",
    "        for isDeleteEnglishStopwords in possibilities :\n",
    "            for isLower in possibilities :\n",
    "                for isLemmatize in possibilities :\n",
    "\n",
    "                    print(\"preprocessing\")\n",
    "\n",
    "                    # Preprocessing datas\n",
    "                    AO_observations = preprocessingAO(AO_extract)\n",
    "                    DC_observations = preprocessingDC(DC_extract)\n",
    "\n",
    "                    title = \"Mincount_10\" \\\n",
    "                    + \" Files_\" + str(len(DC_observations) + len(AO_observations)) \\\n",
    "                    + \" Perplexity\" + str(perplexity) \\\n",
    "                    + \" FrenchStopword_\" + str(isDeleteFrenchStopwords) \\\n",
    "                    + \" EnglishStopword_\" + str(isDeleteEnglishStopwords) \\\n",
    "                    + \" Lower_\" + str(isLower) \\\n",
    "                    + \" Lemmatization_\" + str(isLemmatize) \\\n",
    "                    + \".png\"\n",
    "\n",
    "                    print (title)\n",
    "\n",
    "\n",
    "\n",
    "                    # Concat AO & DC\n",
    "                    listToReduce = AO_observations['sentences2'].tolist() + DC_observations['sentences2'].tolist()\n",
    "\n",
    "                    print(\"corpus\")\n",
    "                    # Plot model and save fig\n",
    "                    corpus = reduceDimensionList(listToReduce)\n",
    "                    model = word2vec.Word2Vec(corpus, min_count=20, seed = 0)\n",
    "\n",
    "                    print(\"plot\")\n",
    "                    tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingGensim(observations):\n",
    "    logging.info('Begin preprocessingGensim')\n",
    "    \n",
    "    \n",
    "    observations['tf-idf'] = \"\"\n",
    "\n",
    "    \n",
    "    # Create a Corpus\n",
    "    dictionary = Dictionary(observations[\"lemmatized\"].tolist())\n",
    "    corpus = [dictionary.doc2bow(text) for text in observations['lemmatized'].tolist()]\n",
    "        \n",
    "    # Create a new TfidfModel using the corpus\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    \n",
    "    for index, row in observations.iterrows():\n",
    "        \n",
    "        tfidf_weights = tfidf[corpus[index]]\n",
    "        sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "        observations.loc[index, 'tf-idf'] = sorted_tfidf_weights\n",
    "        \n",
    "\n",
    "    logging.info('End preprocessingGensim')\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingSpacy(observations):\n",
    "    logging.info('Begin preprocessingSpaCy')\n",
    "    \n",
    "    \n",
    "    observations['nlp'] = \"\"\n",
    "    \n",
    "    # Instantiate the nlp\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    for index, row in observations.iterrows():\n",
    "        \n",
    "        # Tokenization\n",
    "        nlp = nlp(observations.loc[index, 'text'])\n",
    "        observations.loc[index, 'nlp'] = nlp\n",
    "\n",
    "    \n",
    "    logging.info('End preprocessingSpaCy')\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strptime() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-489-945c31c9fbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"March 2011\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mstr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mstr1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: strptime() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "str1 = datetime.strptime(list1, )\n",
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "yourdate = dateutil.parser.parse(list1)\n",
    "\n",
    "list1 = \"06/2011\"\n",
    "yourdate2 = dateutil.parser.parse(list1)\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "yourdate = dateutil.parser.parse(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 6, 17, 0, 0)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourdate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourdate.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "import dateparser # $ pip install dateparser\n",
    "\n",
    "for date_string in [u\"Aujourd'hui\", \"3 juillet\", u\"4 Août\", u\"Hier\", \"06/2012\"]:\n",
    "    print(dateparser.parse(date_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mai', '2014', 'décembre', '2018', 'préfecture', 'de', 'rabat', 'minist', 're', 'de', 'l', 'intérieur', 'maroc']\n"
     ]
    }
   ],
   "source": [
    "#TU de Get date\n",
    "\n",
    "year_pattern1 = '((19|20)\\d{2})'\n",
    "year_pattern2 = '(\\d{2})'\n",
    "\n",
    "month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "month_pattern2 = \"(jan|fev|fév|mar|avr|mai|juin|juil|aout|sep|oct|nov|dec|déc)\"\n",
    "month_pattern3 = \"(janvier|février|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre|décembre) \"\n",
    "month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "\n",
    "p1 = '06/2018 – 12/18 – Consultante – JEMS Group – Banque – Paris, France'\n",
    "p2 = '06/2018-12/2018 – Mission : Chef de Projet Data Gouvernance – BNPParibas CIB'\n",
    "p3 = 'Envt technique :  C# (version 4 et 5) & ASP.NET, SQL server 2008, Visual Studio 2015'\n",
    "p4 = 'Windows Server 2012, 2008 et 2003'\n",
    "p5 = 'Reprise des études pour intégrer les versions 2003 de Microsoft versus les versions 2000'\n",
    "p6 = 'Janvier 2018 – Déc 18 – Consultante – JEMS Group – Banque – Paris, France'\n",
    "p7 = 'Janvier 2018 – Féc 18 – Consultante – JEMS Group – Banque – Paris, France'\n",
    "p8 = '2018 – Consultante – JEMS Group – Banque – Paris, France'\n",
    "p9 = '2018-2019 – Consultante – JEMS Group – Banque – Paris, France'\n",
    "p10 = \"Mai 2014 à Décembre 2018 – Préfecture de RABAT, Ministère de l’Intérieur - MAROC\"\n",
    "\n",
    "text = p10.lower()\n",
    "text = re.split(\"[^A-Za-z0-9é]+\", text)\n",
    "print(text)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    \n",
    "    # 01/2018 -> 02/2018 or Jan/2018 -> Fév/2018\n",
    "    if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(month_pattern, text[i - 1])) :\n",
    "        results.append(\"01/\" + text[i - 1] + \"/\" + text[i])\n",
    "        \n",
    "    # YYYY -> YYYY\n",
    "    elif (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(year_pattern1, text[i - 1])) :\n",
    "        results.append(\"01/\" + \"01/\" + text[i - 1])\n",
    "        results.append(\"01/\" + \"01/\" + text[i])\n",
    "        \n",
    "    # 2018 \n",
    "    elif (i == 0) and (re.match(year_pattern1, text[i])) :\n",
    "        results.append(\"01/\" + \"01/\" + text[i])\n",
    "\n",
    "    \n",
    "\n",
    "if len(results) == 1 :\n",
    "    results.append(\"Aujourd'hui\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/mai/2014'"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 5, 1, 0, 0)"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/décembre/2018'"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 1, 0, 0)"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[1]).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPattern():\n",
    "    \n",
    "    month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "    month_pattern2 = \"(Jan|Fev|Fév|Mar|Avr|Mai|Juin|Juil|Aout|Sep|Oct|Nov|Dec|Déc)\"\n",
    "    month_pattern3 = \"(Janvier|Février|Fevrier|Mars|Avril|Mai|Juin|Juillet|Aout|Septembre|Octobre|Novembre|Decembre|Décembre) \"\n",
    "    month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "    separator_pattern = \"(\\/|-|\\.| )\"\n",
    "    \n",
    "    year_pattern = '((19|20)\\d{2})'\n",
    "\n",
    "    pattern = \"(\" + \"(\" + month_pattern + separator_pattern + year_pattern + \")\" + \\\n",
    "            \"|\" + \"(\" + year_pattern +  \")\" + \")\"\n",
    "\n",
    "\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02'"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"02\".capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\".capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36NLP",
   "language": "python",
   "name": "p36nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
