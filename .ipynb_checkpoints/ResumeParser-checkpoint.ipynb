{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dateparser'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-511-8ea4b5ff86d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdateparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dateparser'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import yaml\n",
    "import dateparser\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# PDFMiner\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "# Collections\n",
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "\n",
    "# Gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# creuser la prÃ©sence de mot anglais\n",
    "\n",
    "# Import for progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vars & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ncharbit\\AppData\\Local\\Continuum\\anaconda3\\envs\\p36NLP\\lib\\site-packages\\ipykernel_launcher.py:11: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "AO_directory = \"data/input/AO\"\n",
    "DC_directory = \"data/input/DC\"\n",
    "\n",
    "AO_output_directory = \"data/output/AO\"\n",
    "AO_save_file = \"AO_save_file.csv\"\n",
    "\n",
    "DC_output_directory = \"data/output/DC\"\n",
    "DC_save_file = \"DC_save_file.csv\"\n",
    "\n",
    "confs_path = \"confs/config.yaml\"\n",
    "confs = yaml.load(open(confs_path, encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete french stopwords\n",
    "isDeleteFrenchStopwords = False\n",
    "\n",
    "# Delete english stopwords\n",
    "isDeleteEnglishStopwords = False \n",
    "\n",
    "#lower the word\n",
    "isLower = True\n",
    "\n",
    "#lemmatize the word\n",
    "isLemmatize = False\n",
    "\n",
    "perplexity=30\n",
    "\n",
    "possibilities = [False, True]\n",
    "\n",
    "perplexityPossibilities=[30, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract & Convert into PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf(path):\n",
    "    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    with open(path, \"rb\") as fp:\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,\n",
    "                                      password=password,\n",
    "                                      caching=caching,\n",
    "                                      check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        text = retstr.getvalue()\n",
    "\n",
    "        fp.close()\n",
    "        device.close()\n",
    "        retstr.close()\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def extract(directory):\n",
    "\n",
    "    # Reference variables\n",
    "    file_agg = list()\n",
    "\n",
    "    # Create list of candidate files\n",
    "    for root, subdirs, files in os.walk(directory):\n",
    "        folder_files = map(lambda x: os.path.join(root, x), files)\n",
    "        file_agg.extend(folder_files)\n",
    "\n",
    "    # Convert list to a pandas DataFrame\n",
    "    observations = pd.DataFrame(data=file_agg, columns=['file_path'])\n",
    "    observations['extension'] = observations['file_path'].apply(lambda x: os.path.splitext(x)[1])\n",
    "    observations['text'] = observations['file_path'].apply(convert_pdf)\n",
    "    \n",
    "    return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AO_observations = extract(AO_directory)\n",
    "DC_observations = extract(DC_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Extracted & Converted files into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(observations, output_directory, save_file):\n",
    "\n",
    "    output_path = os.path.join(output_directory, save_file)\n",
    "    observations.to_csv(path_or_buf=output_path, index_label='index')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(AO_observations, AO_output_directory, AO_save_file)\n",
    "#save(DC_observations, DC_output_directory, DC_save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceDimensionList (my_list) :\n",
    "    \n",
    "    my_list = [item for sublist in my_list for item in sublist]\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumn(start_index, end_index, sentences):\n",
    "\n",
    "        if start_index != 0:\n",
    "            start_index = start_index + 1\n",
    "        \n",
    "        if end_index == 0 :\n",
    "            text = sentences[start_index:]\n",
    "        else : \n",
    "            text = sentences[start_index:end_index]\n",
    "                        \n",
    "        return text \n",
    "    \n",
    "        text = [word_tokenize(w) for w in text]\n",
    "\n",
    "        my_text = []\n",
    "        for sent in text :\n",
    "            my_sent = []\n",
    "            for word in sent :\n",
    "                # Delete date or phone number\n",
    "                #if not len(re.findall(\"[a-zA-Z]\",word)) > 0: \n",
    "                #    continue\n",
    "                    \n",
    "                #lower the word\n",
    "                if isLower:    \n",
    "                    word = word.lower()\n",
    "                    \n",
    "                #lemmatize the word\n",
    "                if isLemmatize:\n",
    "                    word = nlp(word)[0].lemma_ \n",
    "                    \n",
    "                # Delete french stopwords\n",
    "                if isDeleteFrenchStopwords and (word in stopwords.words(\"french\")):\n",
    "                    continue\n",
    "                \n",
    "                # Delete english stopwords\n",
    "                if isDeleteEnglishStopwords and (word in stopwords.words(\"english\")):\n",
    "                    continue\n",
    "                   \n",
    "                #append (or not) the word to the \"sentence\"\n",
    "                if len(word) > 0:\n",
    "                    my_sent.append(word)\n",
    "\n",
    "            #append (or not) the sentence to the \"text\"\n",
    "            if len(my_sent) > 0:\n",
    "                my_text.append(my_sent)\n",
    "                \n",
    "        \n",
    "        return my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingAO(observations):  \n",
    "       \n",
    "    observations['sentences'] = \"\"\n",
    "    observations['sentences2'] = \"\"\n",
    "\n",
    "    for index, row in tqdm(observations.iterrows()):\n",
    "        \n",
    "        # Tokenization\n",
    "        sentences = []        \n",
    "        del_1 = \"\\n\"\n",
    "        delimiters = del_1 \n",
    "        paragraphs = [p for p in re.split(delimiters, row[\"text\"]) if p]\n",
    "        for paragraph in paragraphs:\n",
    "            sentences += sent_tokenize(paragraph)\n",
    "        observations.loc[index, \"sentences\"] = sentences\n",
    "        \n",
    "        observations.loc[index, \"sentences2\"] = getColumn(0, 0, sentences)\n",
    "\n",
    "        \n",
    "    return observations\n",
    "\n",
    "\n",
    "def preprocessingDC(observations):  \n",
    "    \n",
    "    observations['sentences'] = \"\"\n",
    "    observations['sentences2'] = \"\"\n",
    "    observations['skills'] = \"\"\n",
    "    observations['exp'] = \"\"\n",
    "    observations['formation'] = \"\"\n",
    "    \n",
    "        \n",
    "    for index, row in tqdm(observations.iterrows()):\n",
    "        \n",
    "        # Tokenization\n",
    "        sentences = []        \n",
    "        del_1 = \"JEMS - \\d.*.com\"\n",
    "        del_2 = \"Votre contact JEMS .* \\n\"\n",
    "        del_3 = \"\\n\"\n",
    "        #del_4 = \"\\/\"\n",
    "        delimiters = del_1 + '|' + del_2 + '|' + del_3# + '|' + del_4\n",
    "        paragraphs = [p for p in re.split(delimiters, row[\"text\"]) if p]\n",
    "        for paragraph in paragraphs:\n",
    "            sentences += sent_tokenize(paragraph)\n",
    "        observations.loc[index, 'sentences'] = sentences\n",
    "        \n",
    "        \n",
    "        # Get index\n",
    "        list_index_synthese = ['SYNTHESE DE COMPETENCES']\n",
    "        list_index_experience = ['EXPERIENCES PROFESSIONNELLES', 'EXPERIENCE PROFESSIONNELLE']\n",
    "        list_index_formation = ['FORMATIONS', 'FORMATION', \"DIPLOMES & FORMATIONS\"]\n",
    "        for i,e in enumerate(sentences):\n",
    "            if e in list_index_synthese:\n",
    "                index_synthese = i\n",
    "            elif e in list_index_experience:\n",
    "                index_experience = i\n",
    "            elif e in list_index_formation:\n",
    "                index_formation = i\n",
    "\n",
    "        \n",
    "        # Get Section By Section\n",
    "        dictionnaryIndex = {\n",
    "            \"skills\" : index_synthese,\n",
    "            \"exp\" : index_experience,\n",
    "            \"formation\" : index_formation}       \n",
    "        listofTuples = sorted(dictionnaryIndex.items(), key=lambda x: x[1])\n",
    "        for i in range(len(listofTuples)):\n",
    "            k = listofTuples[i][0]\n",
    "            v = int(listofTuples[i][1])\n",
    "            \n",
    "            v2 = 0\n",
    "            if i < len(listofTuples) - 1:\n",
    "                k2 = listofTuples[i+1][0]\n",
    "                v2 = int(listofTuples[i+1][1])\n",
    "            \n",
    "            \n",
    "            observations.loc[index, k] = getColumn(v, v2, sentences)\n",
    "            \n",
    "        observations.loc[index, \"sentences2\"] = getColumn(0, 0, sentences)\n",
    "        \n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:00, 87.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#AO_observations = preprocessingAO(AO_observations)\n",
    "DC_observations = preprocessingDC(DC_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>extension</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>sentences2</th>\n",
       "      <th>skills</th>\n",
       "      <th>exp</th>\n",
       "      <th>formation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences  JEMS Gr...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\nSenior PMO  \\n\\nS...</td>\n",
       "      <td>[1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...</td>\n",
       "      <td>[1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...</td>\n",
       "      <td>[-  CompÃ©tences fonctionnelles :, -  Conduite ...</td>\n",
       "      <td>[06/2018 â 12/18 â Consultante â JEMS Group â ...</td>\n",
       "      <td>[â  2018 â Design thinking project management ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/input/DC\\Dossier de compeÌtences de JEMS ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>RHA \\n\\nConsultant BIG DATA \\n\\nSYNTHESE DE CO...</td>\n",
       "      <td>[RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...</td>\n",
       "      <td>[RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...</td>\n",
       "      <td>[  Savoir-faire :, -  Connaissance de lâÃ©cosys...</td>\n",
       "      <td>[Depuis Avril 2018       JEMS Datafactory, ---...</td>\n",
       "      <td>[DiplÃ´me dâingÃ©nieur en gÃ©nie mathÃ©matique opt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS  - ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nBSE \\n\\nConsultant Dat...</td>\n",
       "      <td>[1, BSE, Consultant Data SÃ©nior, DÃ©veloppement...</td>\n",
       "      <td>[1, BSE, Consultant Data SÃ©nior, DÃ©veloppement...</td>\n",
       "      <td>[Scikit-learn, pandas , pyTorch/ fastai, Keras...</td>\n",
       "      <td>[                                             ...</td>\n",
       "      <td>[2006, 1987, Master2 MathÃ©matiques Fondamental...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/input/DC\\Dossier de compeÌtences JEMS -  ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n...</td>\n",
       "      <td>[IngÃ©nieur dâÃ©tudes confirmÃ© ASP.Net / C#, 7 a...</td>\n",
       "      <td>[IngÃ©nieur dâÃ©tudes confirmÃ© ASP.Net / C#, 7 a...</td>\n",
       "      <td>[SonarQube, Serveur dâapplication :   IIS, Apa...</td>\n",
       "      <td>[Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABA...</td>\n",
       "      <td>[2014/2016  :  MASTER Scientifique (Msc) Optio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/input/DC\\Dossier de compeÌtences JEMS -  ...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...</td>\n",
       "      <td>[1, ., Consultant Technico-Fonctionnel, SYNTHE...</td>\n",
       "      <td>[1, ., Consultant Technico-Fonctionnel, SYNTHE...</td>\n",
       "      <td>[â  Finance, Finance de marchÃ©, Gestion dâacti...</td>\n",
       "      <td>[ AXA Investment Partners â Front office Analy...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - A...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         \\n\\n \\n\\...</td>\n",
       "      <td>[1, DÃ©veloppeur Fullstack, 6 annÃ©es dâexpÃ©rien...</td>\n",
       "      <td>[1, DÃ©veloppeur Fullstack, 6 annÃ©es dâexpÃ©rien...</td>\n",
       "      <td>[MÃ©tiers, Fonctionnelles, Etudes transverses f...</td>\n",
       "      <td>[Novembre 2017 â Mars 2019, DÃ©veloppeur Full S...</td>\n",
       "      <td>[\f",
       "2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - A...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nASA \\n\\nDATA SCIENTIST...</td>\n",
       "      <td>[1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[Domaines de compÃ©tences, Intelligence Artific...</td>\n",
       "      <td>[&lt;IN-TEAM/ startup&gt;, Domaine de compÃ©tences : ...</td>\n",
       "      <td>[ï¨  2011-2013 :   DiplÃ´me national dâingÃ©nieur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - D...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\nDK \\n\\nConsultant Big Data \\n\\...</td>\n",
       "      <td>[1, DK, Consultant Big Data, SYNTHESE DE COMPE...</td>\n",
       "      <td>[1, DK, Consultant Big Data, SYNTHESE DE COMPE...</td>\n",
       "      <td>[â  CompÃ©tences techniques :, - Programmation ...</td>\n",
       "      <td>[FÃ©vrier 2018 â FÃ©vrier 2019 : Data Scientist ...</td>\n",
       "      <td>[â  Juin 2001:, BaccalaurÃ©at grec ''Apolytirio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - E...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         ...</td>\n",
       "      <td>[1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...</td>\n",
       "      <td>[1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...</td>\n",
       "      <td>[  CompÃ©tences techniques, â¢  Langages : Pytho...</td>\n",
       "      <td>[2019 â Consultant Big Data (Consultant Jems) ...</td>\n",
       "      <td>[OBJECTIFS :, â¢ DÃ©finition du pÃ©rimÃ¨tre du Big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - E...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n...</td>\n",
       "      <td>[1, EZA, Consultant DevOps, SYNTHESE DE COMPET...</td>\n",
       "      <td>[1, EZA, Consultant DevOps, SYNTHESE DE COMPET...</td>\n",
       "      <td>[â  CompÃ©tences techniques :, SystÃ¨me OS :    ...</td>\n",
       "      <td>[Octobre 2018 â  Janvier 2019 : Consultant DEV...</td>\n",
       "      <td>[â  Participation au dÃ©veloppement de nouvelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - H...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nH \\n\\nData Scientist  ...</td>\n",
       "      <td>[1, H, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[1, H, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[â  CompÃ©tences techniques : Python(Tensorflow...</td>\n",
       "      <td>[Mai â Novembre 2018 â Data Scientist â BNP Pa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - H...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\nExpert Big Data  ...</td>\n",
       "      <td>[1, Expert Big Data, SYNTHESE DE COMPETENCES, ...</td>\n",
       "      <td>[1, Expert Big Data, SYNTHESE DE COMPETENCES, ...</td>\n",
       "      <td>[CompÃ©tences fonctionnelles :,   Big Data : Ma...</td>\n",
       "      <td>[Depuis Octobre 2015 : Jems group, Expertise B...</td>\n",
       "      <td>[Certification HDP 2.4, Formations BigData (Ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - I...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nIS  \\n\\nData Scientist...</td>\n",
       "      <td>[1, IS, Data Scientist, SYNTHESE DE COMPETENCE...</td>\n",
       "      <td>[1, IS, Data Scientist, SYNTHESE DE COMPETENCE...</td>\n",
       "      <td>[â  CompÃ©tences techniques :,           â¢ Pyth...</td>\n",
       "      <td>[1er sept. 2018  â  1er mars 2019 Data Scienti...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - N...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nNZE \\n\\nCONSULTANT TAL...</td>\n",
       "      <td>[1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...</td>\n",
       "      <td>[1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...</td>\n",
       "      <td>[MISE EN ÅUVRE ET MAINTENANCE DâAPPLICATIONS Q...</td>\n",
       "      <td>[DE NOVEMBRE 2017 A AJOURDâHUI â VEOLIA WATER ...</td>\n",
       "      <td>[Formation ServiceNow, JIRA, Formation Google ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - P...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nP \\n\\nData Scientist \\...</td>\n",
       "      <td>[1, P, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[1, P, Data Scientist, SYNTHESE DE COMPETENCES...</td>\n",
       "      <td>[â  CompÃ©tences techniques :, â, - Programmati...</td>\n",
       "      <td>[28/06/2018 â 31/12/2018 â DATA SCIENTIST â BN...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n \\n\\nArchitecte B...</td>\n",
       "      <td>[1, Architecte Big Data / IA / Cloud, SYNTHESE...</td>\n",
       "      <td>[1, Architecte Big Data / IA / Cloud, SYNTHESE...</td>\n",
       "      <td>[SystÃ¨mes et dÃ©veloppement :, Big Data:, BI:, ...</td>\n",
       "      <td>[02/2018 Ã  31/12/2018   Architecte Entreprise ...</td>\n",
       "      <td>[MIT Sloan &amp; MIT CSAIL Artificial Intelligence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nRDB \\n\\n \\n\\nConsultan...</td>\n",
       "      <td>[1, RDB, Consultant AMOA, Chef de projet fonct...</td>\n",
       "      <td>[1, RDB, Consultant AMOA, Chef de projet fonct...</td>\n",
       "      <td>[Management dâÃ©quipe et pilotage, Coordination...</td>\n",
       "      <td>[Product Owner, Juin 2014 â Mai 2017, Bionexo ...</td>\n",
       "      <td>[Gestion, stratÃ©gie et Ã©conomie dâentreprise (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - R...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\nRTS \\n\\nConsultant Sen...</td>\n",
       "      <td>[1, RTS, Consultant Senior Java/Jee â Batch - ...</td>\n",
       "      <td>[1, RTS, Consultant Senior Java/Jee â Batch - ...</td>\n",
       "      <td>[Base de donnÃ©es, PL SQL, Oracle, Sybase, SQL ...</td>\n",
       "      <td>[SOCIETE GENERALE (VAL DE FONTENAY)  Mars 2018...</td>\n",
       "      <td>[2018 â 2019, Formation Certifiante Expert Dev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS - Z...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n \\n\\n \\n\\n \\n\\n         \\n\\n \\n\\nZAO \\n\\...</td>\n",
       "      <td>[1, ZAO, Data Scientist / Engineer, SYNTHESE D...</td>\n",
       "      <td>[1, ZAO, Data Scientist / Engineer, SYNTHESE D...</td>\n",
       "      <td>[Langages de programmation :, Python (Niveau a...</td>\n",
       "      <td>[Depuis le 11/2018 Ã  Aujourdâhui :, Co-fondate...</td>\n",
       "      <td>[UniversitÃ© PanthÃ©on ASSAS/UniversitÃ© Paris DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS -A.pdf</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...</td>\n",
       "      <td>[1, ., Consultant Big Data, SYNTHESE DE COMPET...</td>\n",
       "      <td>[1, ., Consultant Big Data, SYNTHESE DE COMPET...</td>\n",
       "      <td>[CompÃ©tences techniques :, * OS : Windows, Lin...</td>\n",
       "      <td>[Contexte client : au sein du dÃ©partement Data...</td>\n",
       "      <td>[Formation Big Data | Hadoop | Spark scala, Â« ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS G M...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n \\n\\n \\n\\n \\n\\nChef de Projet Data Manage...</td>\n",
       "      <td>[Chef de Projet Data Management, +10y experien...</td>\n",
       "      <td>[Chef de Projet Data Management, +10y experien...</td>\n",
       "      <td>[Conduite de projet, -, -, -, -, -, -, DÃ©finit...</td>\n",
       "      <td>[Loxam â 2013 - 2018, Missions :, -  Responsab...</td>\n",
       "      <td>[-  MASTER II MIAGE : IngÃ©nierie des SystÃ¨mes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>data/input/DC\\Dossier de CompeÌtences JEMS Gro...</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>\\n\\n          \\n\\n \\n\\n \\n\\nConsultant Big Da...</td>\n",
       "      <td>[Consultant Big Data, 1, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[Consultant Big Data, 1, SYNTHESE DE COMPETENC...</td>\n",
       "      <td>[â  Savoir-faire :, -  Recueil du besoin mÃ©tie...</td>\n",
       "      <td>[Depuis Avril 2017 : SEPHORA, Chef de Projet B...</td>\n",
       "      <td>[-  Test des diffÃ©rentes solutions dâintÃ©grati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            file_path extension  \\\n",
       "0   data/input/DC\\Dossier de CompeÌtences  JEMS Gr...      .pdf   \n",
       "1   data/input/DC\\Dossier de compeÌtences de JEMS ...      .pdf   \n",
       "2   data/input/DC\\Dossier de CompeÌtences JEMS  - ...      .pdf   \n",
       "3   data/input/DC\\Dossier de compeÌtences JEMS -  ...      .pdf   \n",
       "4   data/input/DC\\Dossier de compeÌtences JEMS -  ...      .pdf   \n",
       "5   data/input/DC\\Dossier de CompeÌtences JEMS - A...      .pdf   \n",
       "6   data/input/DC\\Dossier de CompeÌtences JEMS - A...      .pdf   \n",
       "7   data/input/DC\\Dossier de CompeÌtences JEMS - D...      .pdf   \n",
       "8   data/input/DC\\Dossier de CompeÌtences JEMS - E...      .pdf   \n",
       "9   data/input/DC\\Dossier de CompeÌtences JEMS - E...      .pdf   \n",
       "10  data/input/DC\\Dossier de CompeÌtences JEMS - H...      .pdf   \n",
       "11  data/input/DC\\Dossier de CompeÌtences JEMS - H...      .pdf   \n",
       "12  data/input/DC\\Dossier de CompeÌtences JEMS - I...      .pdf   \n",
       "13  data/input/DC\\Dossier de CompeÌtences JEMS - N...      .pdf   \n",
       "14  data/input/DC\\Dossier de CompeÌtences JEMS - P...      .pdf   \n",
       "15  data/input/DC\\Dossier de CompeÌtences JEMS - R...      .pdf   \n",
       "16  data/input/DC\\Dossier de CompeÌtences JEMS - R...      .pdf   \n",
       "17  data/input/DC\\Dossier de CompeÌtences JEMS - R...      .pdf   \n",
       "18  data/input/DC\\Dossier de CompeÌtences JEMS - Z...      .pdf   \n",
       "19  data/input/DC\\Dossier de CompeÌtences JEMS -A.pdf      .pdf   \n",
       "20  data/input/DC\\Dossier de CompeÌtences JEMS G M...      .pdf   \n",
       "21  data/input/DC\\Dossier de CompeÌtences JEMS Gro...      .pdf   \n",
       "\n",
       "                                                 text  \\\n",
       "0   1 \\n\\n         \\n\\n \\n\\n \\n\\nSenior PMO  \\n\\nS...   \n",
       "1   RHA \\n\\nConsultant BIG DATA \\n\\nSYNTHESE DE CO...   \n",
       "2   1 \\n\\n         \\n\\n \\n\\nBSE \\n\\nConsultant Dat...   \n",
       "3    \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n \\n\\n...   \n",
       "4   1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...   \n",
       "5   1 \\n\\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         \\n\\n \\n\\...   \n",
       "6   1 \\n\\n         \\n\\n \\n\\nASA \\n\\nDATA SCIENTIST...   \n",
       "7   1 \\n\\n \\n\\n \\n\\nDK \\n\\nConsultant Big Data \\n\\...   \n",
       "8   1 \\n\\n \\n\\n \\n \\n\\n \\n\\n \\n\\n \\n \\n\\n         ...   \n",
       "9   1 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n...   \n",
       "10  1 \\n\\n         \\n\\n \\n\\nH \\n\\nData Scientist  ...   \n",
       "11  1 \\n\\n         \\n\\n \\n\\n \\n\\nExpert Big Data  ...   \n",
       "12  1 \\n\\n         \\n\\n \\n\\nIS  \\n\\nData Scientist...   \n",
       "13  1 \\n\\n         \\n\\n \\n\\nNZE \\n\\nCONSULTANT TAL...   \n",
       "14  1 \\n\\n         \\n\\n \\n\\nP \\n\\nData Scientist \\...   \n",
       "15  1 \\n\\n         \\n\\n \\n\\n \\n\\n \\n\\nArchitecte B...   \n",
       "16  1 \\n\\n         \\n\\n \\n\\nRDB \\n\\n \\n\\nConsultan...   \n",
       "17  1 \\n\\n         \\n\\n \\n\\nRTS \\n\\nConsultant Sen...   \n",
       "18  1 \\n\\n \\n\\n \\n\\n \\n\\n         \\n\\n \\n\\nZAO \\n\\...   \n",
       "19  1 \\n\\n         \\n\\n \\n\\n \\n\\n. \\n\\nConsultant ...   \n",
       "20   \\n\\n \\n\\n \\n\\n \\n\\nChef de Projet Data Manage...   \n",
       "21   \\n\\n          \\n\\n \\n\\n \\n\\nConsultant Big Da...   \n",
       "\n",
       "                                            sentences  \\\n",
       "0   [1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...   \n",
       "1   [RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...   \n",
       "2   [1, BSE, Consultant Data SÃ©nior, DÃ©veloppement...   \n",
       "3   [IngÃ©nieur dâÃ©tudes confirmÃ© ASP.Net / C#, 7 a...   \n",
       "4   [1, ., Consultant Technico-Fonctionnel, SYNTHE...   \n",
       "5   [1, DÃ©veloppeur Fullstack, 6 annÃ©es dâexpÃ©rien...   \n",
       "6   [1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...   \n",
       "7   [1, DK, Consultant Big Data, SYNTHESE DE COMPE...   \n",
       "8   [1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...   \n",
       "9   [1, EZA, Consultant DevOps, SYNTHESE DE COMPET...   \n",
       "10  [1, H, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "11  [1, Expert Big Data, SYNTHESE DE COMPETENCES, ...   \n",
       "12  [1, IS, Data Scientist, SYNTHESE DE COMPETENCE...   \n",
       "13  [1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...   \n",
       "14  [1, P, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "15  [1, Architecte Big Data / IA / Cloud, SYNTHESE...   \n",
       "16  [1, RDB, Consultant AMOA, Chef de projet fonct...   \n",
       "17  [1, RTS, Consultant Senior Java/Jee â Batch - ...   \n",
       "18  [1, ZAO, Data Scientist / Engineer, SYNTHESE D...   \n",
       "19  [1, ., Consultant Big Data, SYNTHESE DE COMPET...   \n",
       "20  [Chef de Projet Data Management, +10y experien...   \n",
       "21  [Consultant Big Data, 1, SYNTHESE DE COMPETENC...   \n",
       "\n",
       "                                           sentences2  \\\n",
       "0   [1, Senior PMO, SYNTHESE DE COMPETENCES, -  Co...   \n",
       "1   [RHA, Consultant BIG DATA, SYNTHESE DE COMPETE...   \n",
       "2   [1, BSE, Consultant Data SÃ©nior, DÃ©veloppement...   \n",
       "3   [IngÃ©nieur dâÃ©tudes confirmÃ© ASP.Net / C#, 7 a...   \n",
       "4   [1, ., Consultant Technico-Fonctionnel, SYNTHE...   \n",
       "5   [1, DÃ©veloppeur Fullstack, 6 annÃ©es dâexpÃ©rien...   \n",
       "6   [1, ASA, DATA SCIENTIST, SYNTHESE DE COMPETENC...   \n",
       "7   [1, DK, Consultant Big Data, SYNTHESE DE COMPE...   \n",
       "8   [1, E, Consultant Big Data, Ph.D, SYNTHESE DE ...   \n",
       "9   [1, EZA, Consultant DevOps, SYNTHESE DE COMPET...   \n",
       "10  [1, H, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "11  [1, Expert Big Data, SYNTHESE DE COMPETENCES, ...   \n",
       "12  [1, IS, Data Scientist, SYNTHESE DE COMPETENCE...   \n",
       "13  [1, NZE, CONSULTANT TALEND / QLIK SENSE / QLIK...   \n",
       "14  [1, P, Data Scientist, SYNTHESE DE COMPETENCES...   \n",
       "15  [1, Architecte Big Data / IA / Cloud, SYNTHESE...   \n",
       "16  [1, RDB, Consultant AMOA, Chef de projet fonct...   \n",
       "17  [1, RTS, Consultant Senior Java/Jee â Batch - ...   \n",
       "18  [1, ZAO, Data Scientist / Engineer, SYNTHESE D...   \n",
       "19  [1, ., Consultant Big Data, SYNTHESE DE COMPET...   \n",
       "20  [Chef de Projet Data Management, +10y experien...   \n",
       "21  [Consultant Big Data, 1, SYNTHESE DE COMPETENC...   \n",
       "\n",
       "                                               skills  \\\n",
       "0   [-  CompÃ©tences fonctionnelles :, -  Conduite ...   \n",
       "1   [  Savoir-faire :, -  Connaissance de lâÃ©cosys...   \n",
       "2   [Scikit-learn, pandas , pyTorch/ fastai, Keras...   \n",
       "3   [SonarQube, Serveur dâapplication :   IIS, Apa...   \n",
       "4   [â  Finance, Finance de marchÃ©, Gestion dâacti...   \n",
       "5   [MÃ©tiers, Fonctionnelles, Etudes transverses f...   \n",
       "6   [Domaines de compÃ©tences, Intelligence Artific...   \n",
       "7   [â  CompÃ©tences techniques :, - Programmation ...   \n",
       "8   [  CompÃ©tences techniques, â¢  Langages : Pytho...   \n",
       "9   [â  CompÃ©tences techniques :, SystÃ¨me OS :    ...   \n",
       "10  [â  CompÃ©tences techniques : Python(Tensorflow...   \n",
       "11  [CompÃ©tences fonctionnelles :,   Big Data : Ma...   \n",
       "12  [â  CompÃ©tences techniques :,           â¢ Pyth...   \n",
       "13  [MISE EN ÅUVRE ET MAINTENANCE DâAPPLICATIONS Q...   \n",
       "14  [â  CompÃ©tences techniques :, â, - Programmati...   \n",
       "15  [SystÃ¨mes et dÃ©veloppement :, Big Data:, BI:, ...   \n",
       "16  [Management dâÃ©quipe et pilotage, Coordination...   \n",
       "17  [Base de donnÃ©es, PL SQL, Oracle, Sybase, SQL ...   \n",
       "18  [Langages de programmation :, Python (Niveau a...   \n",
       "19  [CompÃ©tences techniques :, * OS : Windows, Lin...   \n",
       "20  [Conduite de projet, -, -, -, -, -, -, DÃ©finit...   \n",
       "21  [â  Savoir-faire :, -  Recueil du besoin mÃ©tie...   \n",
       "\n",
       "                                                  exp  \\\n",
       "0   [06/2018 â 12/18 â Consultante â JEMS Group â ...   \n",
       "1   [Depuis Avril 2018       JEMS Datafactory, ---...   \n",
       "2   [                                             ...   \n",
       "3   [Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABA...   \n",
       "4   [ AXA Investment Partners â Front office Analy...   \n",
       "5   [Novembre 2017 â Mars 2019, DÃ©veloppeur Full S...   \n",
       "6   [<IN-TEAM/ startup>, Domaine de compÃ©tences : ...   \n",
       "7   [FÃ©vrier 2018 â FÃ©vrier 2019 : Data Scientist ...   \n",
       "8   [2019 â Consultant Big Data (Consultant Jems) ...   \n",
       "9   [Octobre 2018 â  Janvier 2019 : Consultant DEV...   \n",
       "10  [Mai â Novembre 2018 â Data Scientist â BNP Pa...   \n",
       "11  [Depuis Octobre 2015 : Jems group, Expertise B...   \n",
       "12  [1er sept. 2018  â  1er mars 2019 Data Scienti...   \n",
       "13  [DE NOVEMBRE 2017 A AJOURDâHUI â VEOLIA WATER ...   \n",
       "14  [28/06/2018 â 31/12/2018 â DATA SCIENTIST â BN...   \n",
       "15  [02/2018 Ã  31/12/2018   Architecte Entreprise ...   \n",
       "16  [Product Owner, Juin 2014 â Mai 2017, Bionexo ...   \n",
       "17  [SOCIETE GENERALE (VAL DE FONTENAY)  Mars 2018...   \n",
       "18  [Depuis le 11/2018 Ã  Aujourdâhui :, Co-fondate...   \n",
       "19  [Contexte client : au sein du dÃ©partement Data...   \n",
       "20  [Loxam â 2013 - 2018, Missions :, -  Responsab...   \n",
       "21  [Depuis Avril 2017 : SEPHORA, Chef de Projet B...   \n",
       "\n",
       "                                            formation  \n",
       "0   [â  2018 â Design thinking project management ...  \n",
       "1   [DiplÃ´me dâingÃ©nieur en gÃ©nie mathÃ©matique opt...  \n",
       "2   [2006, 1987, Master2 MathÃ©matiques Fondamental...  \n",
       "3   [2014/2016  :  MASTER Scientifique (Msc) Optio...  \n",
       "4                                                  []  \n",
       "5                                                [\n",
       "2]  \n",
       "6   [ï¨  2011-2013 :   DiplÃ´me national dâingÃ©nieur...  \n",
       "7   [â  Juin 2001:, BaccalaurÃ©at grec ''Apolytirio...  \n",
       "8   [OBJECTIFS :, â¢ DÃ©finition du pÃ©rimÃ¨tre du Big...  \n",
       "9   [â  Participation au dÃ©veloppement de nouvelle...  \n",
       "10                                                 []  \n",
       "11  [Certification HDP 2.4, Formations BigData (Ce...  \n",
       "12                                                 []  \n",
       "13  [Formation ServiceNow, JIRA, Formation Google ...  \n",
       "14                                                 []  \n",
       "15  [MIT Sloan & MIT CSAIL Artificial Intelligence...  \n",
       "16  [Gestion, stratÃ©gie et Ã©conomie dâentreprise (...  \n",
       "17  [2018 â 2019, Formation Certifiante Expert Dev...  \n",
       "18  [UniversitÃ© PanthÃ©on ASSAS/UniversitÃ© Paris DA...  \n",
       "19  [Formation Big Data | Hadoop | Spark scala, Â« ...  \n",
       "20  [-  MASTER II MIAGE : IngÃ©nierie des SystÃ¨mes ...  \n",
       "21  [-  Test des diffÃ©rentes solutions dâintÃ©grati...  "
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DC_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPreciseDate(text) : \n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.split(\"[^A-Za-z0-9Ã©]+\", text)\n",
    "    \n",
    "    year_pattern1 = '((19|20)\\d{2})'\n",
    "    year_pattern2 = '(\\d{2})'\n",
    "\n",
    "    month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "    month_pattern2 = \"(jan|fev|fÃ©v|mar|avr|mai|juin|juil|aout|sep|oct|nov|dec|dÃ©c)\"\n",
    "    month_pattern3 = \"(janvier|fÃ©vrier|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre|dÃ©cembre) \"\n",
    "    month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # YYYY -> YYYY\n",
    "    for i in range(len(text)):\n",
    "        if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(year_pattern1, text[i - 1])) :\n",
    "            results.append(\"01/\" + \"01/\" + text[i - 1])\n",
    "            results.append(\"01/\" + \"01/\" + text[i])\n",
    "            \n",
    "    # 01/2018 -> 02/2018 or Jan/2018 -> FÃ©v/2018\n",
    "    if len(results) == 0:\n",
    "        for i in range(len(text)):   \n",
    "            if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(month_pattern, text[i - 1])) :\n",
    "                results.append(\"01/\" + text[i - 1] + \"/\" + text[i])\n",
    "\n",
    "    # 2018 \n",
    "    if len(results) == 0:\n",
    "        for i in range(len(text)):   \n",
    "            if (i == 0) and (re.match(year_pattern1, text[i])) :\n",
    "                results.append(\"01/\" + \"01/\" + text[i])\n",
    "\n",
    "\n",
    "\n",
    "    if len(results) == 1 :\n",
    "        results.append(\"Aujourd'hui\")\n",
    "        \n",
    "\n",
    "    for i in range(len(results)) :\n",
    "        results[i] = dateparser.parse(results[i])\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def getKeyDate(sentences):\n",
    "    \n",
    "    keydate = []\n",
    "    for sentence in sentences : \n",
    "        if len(getPreciseDate(sentence)) > 0 : \n",
    "            keydate.append(sentence)\n",
    "\n",
    "    \n",
    "    if len(keydate) <= 0 :\n",
    "        raise Exception(\"No professionnal experience find\")\n",
    "            \n",
    "    return keydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllInformations(keydate, df_):\n",
    "    \n",
    "    \n",
    "    for i in range(len(keydate) - 1) : \n",
    "        dateDebut = DC_observations.loc[DC, \"exp\"].index(keydate[i])\n",
    "        dateFin = DC_observations.loc[DC, \"exp\"].index(keydate[i + 1])\n",
    "\n",
    "        df_['Mission'] = df_['Mission'].astype(object)\n",
    "        df_['MissionString'] = df_['MissionString'].astype(object)\n",
    "\n",
    "        df_.at[i, \"Mission\"] = DC_observations.loc[DC, \"exp\"][dateDebut:dateFin]\n",
    "        df_.at[i, \"MissionString\"] = \" \".join(df_.at[i, \"Mission\"])\n",
    "\n",
    "        df_.loc[i, \"DD\"] = keydate[i]\n",
    "        df_.loc[i, \"DF\"] = keydate[i + 1]\n",
    "\n",
    "        df_.loc[i, \"iDD\"] = dateDebut\n",
    "        df_.loc[i, \"iDF\"] = dateFin\n",
    "        \n",
    "\n",
    "    for index, row in df_.iterrows():\n",
    "    \n",
    "        text = str(df_.loc[index, \"MissionString\"])\n",
    "\n",
    "        pattern = getPattern()\n",
    "        \n",
    "        # Get start date & end date\n",
    "        mydates = getPreciseDate(text)       \n",
    "\n",
    "        # Parse into real date\n",
    "        df_.loc[index, \"DD\"] = mydates[0]\n",
    "        df_.loc[index, \"DF\"] = mydates[1]\n",
    "\n",
    "    df_['DD'] = pd.to_datetime(df_['DD'])\n",
    "    df_['DF'] = pd.to_datetime(df_['DF'])\n",
    "    df_['result'] = df_[\"DF\"] - df_['DD']\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_count(string_to_search, term):\n",
    "    try:\n",
    "        term = r'\\b' + re.escape(term) + r'\\b'\n",
    "        regular_expression = re.compile(term, re.IGNORECASE)\n",
    "        result = re.findall(regular_expression, string_to_search)\n",
    "        return len(result)\n",
    "    except Exception:\n",
    "        logging.error('Error occurred during regex search')\n",
    "        return 0\n",
    "    \n",
    "def extract_skills(resume_text, extractor, items_of_interest):\n",
    "    potential_skills_dict = dict()\n",
    "    matched_skills = []\n",
    "\n",
    "    # Translate the YAML file into a dic : potential_skills_dict\n",
    "    for skill_input in items_of_interest:\n",
    "\n",
    "        # Format list inputs\n",
    "        if type(skill_input) is list and len(skill_input) >= 1:\n",
    "            potential_skills_dict[skill_input[0]] = skill_input\n",
    "\n",
    "        # Format string inputs\n",
    "        elif type(skill_input) is str:\n",
    "            potential_skills_dict[skill_input] = [skill_input]\n",
    "        else:\n",
    "            logging.warn('Unknown skill listing type: {}. Please format as either a single string or a list of strings'\n",
    "                         ''.format(skill_input))\n",
    "\n",
    "    for (skill_name, skill_alias_list) in potential_skills_dict.items():\n",
    "\n",
    "        skill_matches = 0\n",
    "        # Iterate through aliases\n",
    "        for skill_alias in skill_alias_list:\n",
    "            # Add the number of matches for each alias\n",
    "            skill_matches += term_count(resume_text.lower(), skill_alias.lower())\n",
    "            \n",
    "\n",
    "        # If at least one alias is found, add skill name to set of skills\n",
    "        if skill_matches > 0:\n",
    "            matched_skills.append(skill_name)\n",
    "\n",
    "    return matched_skills\n",
    "\n",
    "def extract_fields(df):\n",
    "    for extractor, items_of_interest in confs['extractors'].items():\n",
    "        df_[extractor] = df_['MissionString'].apply(lambda x: extract_skills(x, extractor, items_of_interest))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "listtmp = [] \n",
    "\n",
    "for DC in range(len(DC_observations)) :\n",
    "#for DC in range(0,1) :\n",
    "\n",
    "    # Get all key date from the experiences\n",
    "    keydate = getKeyDate(DC_observations.loc[DC, \"exp\"])\n",
    "\n",
    "    # Create a tmp dataframe        \n",
    "    df_ = pd.DataFrame(index=np.arange(len(keydate) -1), columns=[\"Mission\", \"MissionString\",  \"DD\", \"DF\", \"iDD\", \"iDF\"])\n",
    "    df_ = df_.fillna(0) # with 0s rather than NaNs\n",
    "    \n",
    "    df_ = getAllInformations(keydate, df_)\n",
    "\n",
    "    df_ = extract_fields(df_)\n",
    "\n",
    "    listtmp.append(df_.copy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mission</th>\n",
       "      <th>MissionString</th>\n",
       "      <th>DD</th>\n",
       "      <th>DF</th>\n",
       "      <th>iDD</th>\n",
       "      <th>iDF</th>\n",
       "      <th>result</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Platforms</th>\n",
       "      <th>Database</th>\n",
       "      <th>...</th>\n",
       "      <th>DataVisualisation</th>\n",
       "      <th>DevOps</th>\n",
       "      <th>Cloud</th>\n",
       "      <th>BackProgramming</th>\n",
       "      <th>FrontProgramming</th>\n",
       "      <th>Methodologie</th>\n",
       "      <th>MachineLearning</th>\n",
       "      <th>Mobile</th>\n",
       "      <th>Langages</th>\n",
       "      <th>Open-source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABA...</td>\n",
       "      <td>Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABAT...</td>\n",
       "      <td>2014-05-01</td>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1675 days</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Windows]</td>\n",
       "      <td>[SQL]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[C, .NET, HTML]</td>\n",
       "      <td>[HTML]</td>\n",
       "      <td>[Agile]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Janvier 2013 Ã  janvier 2016 â Formateur Vacat...</td>\n",
       "      <td>Janvier 2013 Ã  janvier 2016 â Formateur Vacata...</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>1095 days</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[SQL]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[java, C]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Mission  \\\n",
       "0  [Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABA...   \n",
       "1  [Janvier 2013 Ã  janvier 2016 â Formateur Vacat...   \n",
       "\n",
       "                                       MissionString         DD         DF  \\\n",
       "0  Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABAT... 2014-05-01 2018-12-01   \n",
       "1  Janvier 2013 Ã  janvier 2016 â Formateur Vacata... 2013-01-01 2016-01-01   \n",
       "\n",
       "   iDD  iDF    result Experience  Platforms Database  ... DataVisualisation  \\\n",
       "0    0   34 1675 days         []  [Windows]    [SQL]  ...                []   \n",
       "1   34   39 1095 days         []         []    [SQL]  ...                []   \n",
       "\n",
       "  DevOps Cloud  BackProgramming FrontProgramming Methodologie MachineLearning  \\\n",
       "0     []    []  [C, .NET, HTML]           [HTML]      [Agile]              []   \n",
       "1     []    []        [java, C]               []           []              []   \n",
       "\n",
       "  Mobile Langages Open-source  \n",
       "0     []       []          []  \n",
       "1     []       []          []  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 3\n",
    "listtmp[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Windows': Timedelta('1675 days 00:00:00'),\n",
       " 'SQL': Timedelta('2770 days 00:00:00'),\n",
       " 'C': Timedelta('2770 days 00:00:00'),\n",
       " '.NET': Timedelta('1675 days 00:00:00'),\n",
       " 'HTML': Timedelta('3350 days 00:00:00'),\n",
       " 'Agile': Timedelta('1675 days 00:00:00'),\n",
       " 'java': Timedelta('1095 days 00:00:00')}"
      ]
     },
     "execution_count": 910,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = listtmp[i]\n",
    "\n",
    "skills = {}\n",
    "\n",
    "for index, row in df_.iterrows():\n",
    "    for col in df_.loc[:, \"Experience\":] :\n",
    "        \n",
    "        items = df_.at[index, col]\n",
    "        if len(items) == 0 : \n",
    "            continue\n",
    "            \n",
    "        for item in items:\n",
    "            if not item in skills : \n",
    "                skills[item] = df_.at[index, \"result\"]\n",
    "            else :\n",
    "                skills[item] += df_.at[index, \"result\"]\n",
    "                \n",
    "                \n",
    "\n",
    "skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_ = pd.DataFrame(index=np.arange(1))\n",
    "df2_ = df2_.fillna(0) # with 0s rather than NaNs\n",
    "    \n",
    "for key, value in skills.items() :\n",
    "    if value.days < 0 : \n",
    "        df2_[key] = \"Junior\"\n",
    "    else :\n",
    "        df2_[key] = \"Senior\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Windows</th>\n",
       "      <th>Tableau</th>\n",
       "      <th>Python</th>\n",
       "      <th>SQL</th>\n",
       "      <th>Oracle</th>\n",
       "      <th>java</th>\n",
       "      <th>MySQL</th>\n",
       "      <th>HTML</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Junior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Junior</td>\n",
       "      <td>Senior</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Windows Tableau  Python     SQL  Oracle    java   MySQL    HTML\n",
       "0  Junior  Senior  Senior  Junior  Junior  Junior  Senior  Senior"
      ]
     },
     "execution_count": 905,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Display(observations, column, threshold) :\n",
    "\n",
    "    allTexts = reduceDimensionList(observations[column].tolist())\n",
    "    allTexts = reduceDimensionList(allTexts)\n",
    "    \n",
    "    \n",
    "    # Display len of words\n",
    "    test = [len(w) for w in allTexts]\n",
    "    print(\"Display len of words : \")\n",
    "    print(test)\n",
    "    plt.hist(test)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Write word with len > 15\n",
    "    test = [w for w in allTexts if len(w)>15]\n",
    "    if (len(test) > 0):\n",
    "        print(\"Display words where len>15 : \")\n",
    "        print(test)\n",
    "\n",
    "\n",
    "    # Write & Display word frequency >= threshold\n",
    "    test = Counter(allTexts)\n",
    "    test = {x : test[x] for x in test if test[x] >= threshold}\n",
    "    \n",
    "    print(\"Display frequency of words : \")\n",
    "    print(test)\n",
    "    \n",
    "    labels, values = zip(*test.items())\n",
    "    indexes = np.arange(len(labels))\n",
    "    width = 1\n",
    "    plt.bar(indexes, values, width)\n",
    "    plt.xticks(indexes + width * 0.5, labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display(AO_observations, 'sentences2', 5)\n",
    "#Display(DC_observations, 'skills', 4)\n",
    "#Display(DC_observations, 'exp',20)\n",
    "#Display(DC_observations, 'formation', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#listToReduce = AO_observations['sentences2'].tolist() + DC_observations['sentences2'].tolist()\n",
    "\n",
    "#corpus = reduceDimensionList(listToReduce)\n",
    "#model = word2vec.Word2Vec(corpus, min_count=15, seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    # Get words\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    # Get X & Y values\n",
    "    tsne_model = TSNE(perplexity=perplexity, init='pca', n_iter=5000, random_state=0)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "\n",
    "    matrix = np.column_stack((x, y))\n",
    "    clusters_number = 5\n",
    "    kclusterer = KMeansClusterer(clusters_number,  distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "    assigned_clusters = kclusterer.cluster(matrix, assign_clusters=True)\n",
    "\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, clusters_number))\n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i], c=colors[assigned_clusters[i]])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "        \n",
    "        \n",
    "    title = \"Mincount_10\" \\\n",
    "    + \" Files_\" + str(len(DC_observations) + len(AO_observations)) \\\n",
    "    + \" Perplexity\" + str(perplexity) \\\n",
    "    + \" FrenchStopword_\" + str(isDeleteFrenchStopwords) \\\n",
    "    + \" EnglishStopword_\" + str(isDeleteEnglishStopwords) \\\n",
    "    + \" Lower_\" + str(isLower) \\\n",
    "    + \" Lemmatization_\" + str(isLemmatize) \\\n",
    "    + \".png\"\n",
    "\n",
    "                    \n",
    "                    \n",
    "    plt.savefig(title, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tsne_plot(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AO_extract = extract(AO_directory)\n",
    "DC_extract = extract(DC_directory)\n",
    "\n",
    "for perplexity in perplexityPossibilities :\n",
    "    for isDeleteFrenchStopwords in possibilities :\n",
    "        for isDeleteEnglishStopwords in possibilities :\n",
    "            for isLower in possibilities :\n",
    "                for isLemmatize in possibilities :\n",
    "\n",
    "                    print(\"preprocessing\")\n",
    "\n",
    "                    # Preprocessing datas\n",
    "                    AO_observations = preprocessingAO(AO_extract)\n",
    "                    DC_observations = preprocessingDC(DC_extract)\n",
    "\n",
    "                    title = \"Mincount_10\" \\\n",
    "                    + \" Files_\" + str(len(DC_observations) + len(AO_observations)) \\\n",
    "                    + \" Perplexity\" + str(perplexity) \\\n",
    "                    + \" FrenchStopword_\" + str(isDeleteFrenchStopwords) \\\n",
    "                    + \" EnglishStopword_\" + str(isDeleteEnglishStopwords) \\\n",
    "                    + \" Lower_\" + str(isLower) \\\n",
    "                    + \" Lemmatization_\" + str(isLemmatize) \\\n",
    "                    + \".png\"\n",
    "\n",
    "                    print (title)\n",
    "\n",
    "\n",
    "\n",
    "                    # Concat AO & DC\n",
    "                    listToReduce = AO_observations['sentences2'].tolist() + DC_observations['sentences2'].tolist()\n",
    "\n",
    "                    print(\"corpus\")\n",
    "                    # Plot model and save fig\n",
    "                    corpus = reduceDimensionList(listToReduce)\n",
    "                    model = word2vec.Word2Vec(corpus, min_count=20, seed = 0)\n",
    "\n",
    "                    print(\"plot\")\n",
    "                    tsne_plot(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Others preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingGensim(observations):\n",
    "    logging.info('Begin preprocessingGensim')\n",
    "    \n",
    "    \n",
    "    observations['tf-idf'] = \"\"\n",
    "\n",
    "    \n",
    "    # Create a Corpus\n",
    "    dictionary = Dictionary(observations[\"lemmatized\"].tolist())\n",
    "    corpus = [dictionary.doc2bow(text) for text in observations['lemmatized'].tolist()]\n",
    "        \n",
    "    # Create a new TfidfModel using the corpus\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    \n",
    "    for index, row in observations.iterrows():\n",
    "        \n",
    "        tfidf_weights = tfidf[corpus[index]]\n",
    "        sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "        observations.loc[index, 'tf-idf'] = sorted_tfidf_weights\n",
    "        \n",
    "\n",
    "    logging.info('End preprocessingGensim')\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingSpacy(observations):\n",
    "    logging.info('Begin preprocessingSpaCy')\n",
    "    \n",
    "    \n",
    "    observations['nlp'] = \"\"\n",
    "    \n",
    "    # Instantiate the nlp\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    for index, row in observations.iterrows():\n",
    "        \n",
    "        # Tokenization\n",
    "        nlp = nlp(observations.loc[index, 'text'])\n",
    "        observations.loc[index, 'nlp'] = nlp\n",
    "\n",
    "    \n",
    "    logging.info('End preprocessingSpaCy')\n",
    "    return observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strptime() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-489-945c31c9fbf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"March 2011\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mstr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mstr1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: strptime() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "#datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "str1 = datetime.strptime(list1, )\n",
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "yourdate = dateutil.parser.parse(list1)\n",
    "\n",
    "list1 = \"06/2011\"\n",
    "yourdate2 = dateutil.parser.parse(list1)\n",
    "\n",
    "list1 = \"March 2011\"\n",
    "yourdate = dateutil.parser.parse(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 6, 17, 0, 0)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourdate2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yourdate.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n",
      "<class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "import dateparser # $ pip install dateparser\n",
    "\n",
    "for date_string in [u\"Aujourd'hui\", \"3 juillet\", u\"4 AoÃ»t\", u\"Hier\", \"06/2012\"]:\n",
    "    print(dateparser.parse(date_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mai', '2014', 'dÃ©cembre', '2018', 'prÃ©fecture', 'de', 'rabat', 'minist', 're', 'de', 'l', 'intÃ©rieur', 'maroc']\n"
     ]
    }
   ],
   "source": [
    "#TU de Get date\n",
    "\n",
    "year_pattern1 = '((19|20)\\d{2})'\n",
    "year_pattern2 = '(\\d{2})'\n",
    "\n",
    "month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "month_pattern2 = \"(jan|fev|fÃ©v|mar|avr|mai|juin|juil|aout|sep|oct|nov|dec|dÃ©c)\"\n",
    "month_pattern3 = \"(janvier|fÃ©vrier|fevrier|mars|avril|mai|juin|juillet|aout|septembre|octobre|novembre|decembre|dÃ©cembre) \"\n",
    "month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "\n",
    "p1 = '06/2018 â 12/18 â Consultante â JEMS Group â Banque â Paris, France'\n",
    "p2 = '06/2018-12/2018 â Mission : Chef de Projet Data Gouvernance â BNPParibas CIB'\n",
    "p3 = 'Envt technique :  C# (version 4 et 5) & ASP.NET, SQL server 2008, Visual Studio 2015'\n",
    "p4 = 'Windows Server 2012, 2008 et 2003'\n",
    "p5 = 'Reprise des Ã©tudes pour intÃ©grer les versions 2003 de Microsoft versus les versions 2000'\n",
    "p6 = 'Janvier 2018 â DÃ©c 18 â Consultante â JEMS Group â Banque â Paris, France'\n",
    "p7 = 'Janvier 2018 â FÃ©c 18 â Consultante â JEMS Group â Banque â Paris, France'\n",
    "p8 = '2018 â Consultante â JEMS Group â Banque â Paris, France'\n",
    "p9 = '2018-2019 â Consultante â JEMS Group â Banque â Paris, France'\n",
    "p10 = \"Mai 2014 Ã  DÃ©cembre 2018 â PrÃ©fecture de RABAT, MinistÃ¨re de lâIntÃ©rieur - MAROC\"\n",
    "\n",
    "text = p10.lower()\n",
    "text = re.split(\"[^A-Za-z0-9Ã©]+\", text)\n",
    "print(text)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    \n",
    "    # 01/2018 -> 02/2018 or Jan/2018 -> FÃ©v/2018\n",
    "    if (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(month_pattern, text[i - 1])) :\n",
    "        results.append(\"01/\" + text[i - 1] + \"/\" + text[i])\n",
    "        \n",
    "    # YYYY -> YYYY\n",
    "    elif (i != 0) and (re.match(year_pattern1, text[i])) and (re.match(year_pattern1, text[i - 1])) :\n",
    "        results.append(\"01/\" + \"01/\" + text[i - 1])\n",
    "        results.append(\"01/\" + \"01/\" + text[i])\n",
    "        \n",
    "    # 2018 \n",
    "    elif (i == 0) and (re.match(year_pattern1, text[i])) :\n",
    "        results.append(\"01/\" + \"01/\" + text[i])\n",
    "\n",
    "    \n",
    "\n",
    "if len(results) == 1 :\n",
    "    results.append(\"Aujourd'hui\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/mai/2014'"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2014, 5, 1, 0, 0)"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01/dÃ©cembre/2018'"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 1, 0, 0)"
      ]
     },
     "execution_count": 819,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateparser.parse(results[1]).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPattern():\n",
    "    \n",
    "    month_pattern1 = \"(0[1-9]|10|11|12)\"\n",
    "    month_pattern2 = \"(Jan|Fev|FÃ©v|Mar|Avr|Mai|Juin|Juil|Aout|Sep|Oct|Nov|Dec|DÃ©c)\"\n",
    "    month_pattern3 = \"(Janvier|FÃ©vrier|Fevrier|Mars|Avril|Mai|Juin|Juillet|Aout|Septembre|Octobre|Novembre|Decembre|DÃ©cembre) \"\n",
    "    month_pattern = \"(\" + month_pattern1 + \"|\" + month_pattern2 + \"|\" + month_pattern3 + \")\"\n",
    "\n",
    "    separator_pattern = \"(\\/|-|\\.| )\"\n",
    "    \n",
    "    year_pattern = '((19|20)\\d{2})'\n",
    "\n",
    "    pattern = \"(\" + \"(\" + month_pattern + separator_pattern + year_pattern + \")\" + \\\n",
    "            \"|\" + \"(\" + year_pattern +  \")\" + \")\"\n",
    "\n",
    "\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'02'"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"02\".capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello\".capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p36NLP",
   "language": "python",
   "name": "p36nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
